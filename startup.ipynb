{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Interactive Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for a strange bug with compute instances \n",
    "import os\n",
    "\n",
    "os.system('sudo cp /etc/nginx/nginx.conf setup/temp.conf') # stupid\n",
    "\n",
    "nginx = ''\n",
    "\n",
    "with open('setup/temp.conf') as f:\n",
    "    for line in f.readlines():\n",
    "        if 'websocket/|/ws/' in line:\n",
    "            nginx += line.replace('websocket/|/ws/', 'websocket/|/ws')\n",
    "        else:\n",
    "            nginx += line\n",
    "       \n",
    "with open('setup/temp2.conf', 'w') as f:\n",
    "    f.write(nginx)\n",
    "    \n",
    "os.system('sudo mv setup/temp2.conf /etc/nginx/nginx.conf')\n",
    "os.system('sudo service nginx restart')\n",
    "os.system('rm setup/temp.conf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all packages used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception cannot import name '_DistributedTraining'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset, Environment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML setup\n",
    "\n",
    "Get the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='azureml-dask', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='copeters-rg')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your name\n",
    "\n",
    "Enter your name and virtual network information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name: dask\n",
      "\n",
      "vNET RG: copeters-rg\n",
      "vNET name: uksouth-vnet\n",
      "vNET subnet name: default\n",
      "\n",
      "Compute target: dask-dask-ct\n",
      "Experiment name: dask-dask-demo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### name\n",
    "name        = 'dask'             # REPLACE\n",
    "\n",
    "### vnet settings\n",
    "vnet_rg     = ws.resource_group  # replace if needed\n",
    "vnet_name   = 'uksouth-vnet'     # replace if needed\n",
    "subnet_name = 'default'          # replace if needed\n",
    "\n",
    "### azure ml names \n",
    "ct_name     = f'{name}-dask-ct'\n",
    "exp_name    = f'{name}-dask-demo'\n",
    "\n",
    "### trust but verify\n",
    "verify = f'''\n",
    "Name: {name}\n",
    "\n",
    "vNET RG: {vnet_rg}\n",
    "vNET name: {vnet_name}\n",
    "vNET subnet name: {subnet_name}\n",
    "\n",
    "Compute target: {ct_name}\n",
    "Experiment name: {exp_name}\n",
    "'''\n",
    "\n",
    "print(verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VM pool\n",
    "\n",
    "Create Azure ML VM pool for creating remote dask cluster(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute(workspace=Workspace.create(name='azureml-dask', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='copeters-rg'), name=dask-dask-ct, id=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourceGroups/copeters-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-dask/computes/dask-dask-ct, type=AmlCompute, provisioning_state=Succeeded, location=uksouth, tags=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ct_name not in ws.compute_targets:\n",
    "    # create config for Azure ML cluster\n",
    "    # change properties as needed\n",
    "    config = AmlCompute.provisioning_configuration(\n",
    "             vm_size                       = 'STANDARD_D14_V2',  \n",
    "             min_nodes                     = 0,\n",
    "             max_nodes                     = 200,\n",
    "             vnet_resourcegroup_name       = vnet_rg,              \n",
    "             vnet_name                     = vnet_name,            \n",
    "             subnet_name                   = subnet_name,          \n",
    "             idle_seconds_before_scaledown = 300\n",
    "    )\n",
    "    ct = ComputeTarget.create(ws, ct_name, config)\n",
    "    ct.wait_for_completion(show_output=True)    \n",
    "else:\n",
    "    ct = ws.compute_targets[ct_name]\n",
    "    \n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup cluster\n",
    "\n",
    "Start the run now. The first time, this will take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "codefileshare = 'codefileshare'\n",
    "\n",
    "if codefileshare not in ws.datastores:\n",
    "    Datastore.register_azure_file_share(ws, codefileshare,\n",
    "                                        'code-391ff5ac-6576-460f-ba4d-7e03433c68b6',                     # stupid\n",
    "                                        account_name = ws.datastores['workspacefilestore'].account_name, # less stupid\n",
    "                                        account_key  = ws.datastores['workspacefilestore'].account_key   # less less stupid\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "daskdata = 'noaa-isd-files'\n",
    "\n",
    "if daskdata not in ws.datasets:\n",
    "    ds = Dataset.File.from_files('https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/*/*/*.parquet', validate=False)\n",
    "    os.system('sudo chmod 777 /mnt')\n",
    "    ds.download('/mnt/data/isd')\n",
    "    ws.datastores['gen2'].upload('/mnt/data/isd', '/noaa-isd')\n",
    "    ds = ds.register(ws, daskdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>dask-dask-demo</td><td>dask-dask-demo_1580440649_506153b5</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/dask-dask-demo/runs/dask-dask-demo_1580440649_506153b5?wsid=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourcegroups/copeters-rg/workspaces/azureml-dask\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: dask-dask-demo,\n",
       "Id: dask-dask-demo_1580440649_506153b5,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # of nodes \n",
    "nodes = 25; GPU = False  \n",
    "\n",
    "script_params = {\n",
    "    '--jupyter': True,\n",
    "    '--code_store': ws.datastores[codefileshare], \n",
    "    '--data_store': ws.datastores['gen2']         # replace with relevant datastore\n",
    "}\n",
    "\n",
    "if GPU:\n",
    "    script_params['--use_GPU']         = True\n",
    "    script_params['--n_gpus_per_node'] = 4\n",
    "    env_params = {'custom_docker_image': 'todrabas/aml_rapids:latest', 'user_managed': True}\n",
    "else:\n",
    "    env_params = {'pip_packages': ['mpi4py',\n",
    "                                   'distributed',\n",
    "                                   'dask[complete]',\n",
    "                                   'dask-ml[complete]',\n",
    "                                   'fastparquet',\n",
    "                                   'pyarrow',\n",
    "                                   'jupyterlab',\n",
    "                                   'joblib',\n",
    "                                   'notebook',\n",
    "                                   'adlfs', \n",
    "                                   'fsspec', \n",
    "                                   'azureml-sdk[notebooks]',\n",
    "                                   'azureml-dataprep[fuse]',\n",
    "                                   'lz4']}\n",
    "\n",
    "exp   = Experiment(ws, exp_name)\n",
    "est   = Estimator('setup', \n",
    "                  compute_target          = ct, \n",
    "                  entry_script            = 'start.py',          # sets up Dask cluster\n",
    "                  script_params           = script_params,\n",
    "                  node_count              = nodes,        \n",
    "                  distributed_training    = MpiConfiguration(),\n",
    "                  **env_params\n",
    ")\n",
    "\n",
    "#### SET PROPER INTERPRETER - from Tom\n",
    "if GPU:\n",
    "    est._estimator_config.environment.python.interpreter_path = '/opt/conda/envs/rapids/bin/python' \n",
    "\n",
    "#run = next(exp.get_runs()) # use this to get existing run (if kernel restarted, etc)\n",
    "run = exp.submit(est); run.log('nodes', nodes)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737952908d104318841c057980dc2204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/dask-dask-demo/runs/dask-dask-demo_1580440649_506153b5?wsid=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourcegroups/copeters-rg/workspaces/azureml-dask\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"run_properties\": {\"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"created_utc\": \"2020-01-31T03:17:31.875483Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"95cd5c0d-0571-461f-a5d1-f744d712a1e3\", \"azureml.git.repository_uri\": \"http://github.com/lostmygithubaccount/dasky\", \"mlflow.source.git.repoURL\": \"http://github.com/lostmygithubaccount/dasky\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"b9c7488b061268527eaa44542255be1279130237\", \"mlflow.source.git.commit\": \"b9c7488b061268527eaa44542255be1279130237\", \"azureml.git.dirty\": \"False\", \"AzureML.DerivedImageName\": \"azureml/azureml_d6d59190ad2b3a5ef1350d142ca08821\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_1155e9e8bc949f0297c44006b498cf5ff82ce349264014bda0202366ac362c74_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_1155e9e8bc949f0297c44006b498cf5ff82ce349264014bda0202366ac362c74_d.txt?sv=2019-02-02&sr=b&sig=IB1UaKyHf9tNPbJnAS%2FqQzfTcsnTyG9bUUP4cBGzf9o%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_1363d3859a073c8f8a5fe50c10e6f3ab15eefb00d666eec81a7228197d9301d5_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_1363d3859a073c8f8a5fe50c10e6f3ab15eefb00d666eec81a7228197d9301d5_d.txt?sv=2019-02-02&sr=b&sig=4KydReXUypL9olDyN1pzhKOjPbzz89hjxMqfgsLQHuI%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_1a6015ca0a64f0c7b2abcd18c6bf2d39bfbc9b16cc335339e67ea1450c48db33_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_1a6015ca0a64f0c7b2abcd18c6bf2d39bfbc9b16cc335339e67ea1450c48db33_d.txt?sv=2019-02-02&sr=b&sig=zwEP7yioINWQaFx8%2BChGUSsAceSxiIMjecnxpC64rEI%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_24bcd2b885bc2b7a26754b709fc114f686b931cbe3c451d8fd0593cdd8140387_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_24bcd2b885bc2b7a26754b709fc114f686b931cbe3c451d8fd0593cdd8140387_d.txt?sv=2019-02-02&sr=b&sig=JG4QSNGy479jyWNB7zQD8YOyoGaR3Co07H8x%2BXqs%2BBM%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_2757bb30485291f418effccba9e9df670bcb59ff502c6ab3e142272c38ff57e9_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_2757bb30485291f418effccba9e9df670bcb59ff502c6ab3e142272c38ff57e9_d.txt?sv=2019-02-02&sr=b&sig=qWvD%2FIG0BWnTaINEUN3J9LG30HRxtK3RHAsbfIRl7Dc%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_35fe8ed0e816509418f78d86511693a538092c76ffdf596b83fa9dbcff82167f_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_35fe8ed0e816509418f78d86511693a538092c76ffdf596b83fa9dbcff82167f_d.txt?sv=2019-02-02&sr=b&sig=d3Nx1N%2BsPnz814C1jsM1zznhOmTwt7ZFkqp9wZzPHoc%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_43e587ee14b9737fb94fd98b02b62e3cc2855f07e88e87919e134c51d28419cd_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_43e587ee14b9737fb94fd98b02b62e3cc2855f07e88e87919e134c51d28419cd_d.txt?sv=2019-02-02&sr=b&sig=U9AI3wmUdyMm0EQqNfzB0dumlmEf32sPnCzGQH5EH1U%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_489cb80989e58208c697d96047c425f978551633da643ea4b5cfef3acf48925e_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_489cb80989e58208c697d96047c425f978551633da643ea4b5cfef3acf48925e_d.txt?sv=2019-02-02&sr=b&sig=wLbpR%2BvvXQchJvIVxMWDQQ3GeOUAqltzl7Lx3KOdeqs%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_49fa45ca3caf35eff39803c1aeee2b7f459f281dcc55616fdadb09274161fcae_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_49fa45ca3caf35eff39803c1aeee2b7f459f281dcc55616fdadb09274161fcae_d.txt?sv=2019-02-02&sr=b&sig=IN5lycvs784iNwpPxLpaUZCf7PX9fNihxSJ6lApNQ4s%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_4ac5e638ef79cd2db7f02922281f1b82fbcc61a6a6c28dcb7d482025ce5642c9_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_4ac5e638ef79cd2db7f02922281f1b82fbcc61a6a6c28dcb7d482025ce5642c9_d.txt?sv=2019-02-02&sr=b&sig=jgzefdaaeDyEwstC0ZRtcKla88%2FuMLTa2Dg%2BTin902I%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_4ebfbc954e8eaff94ba23fa849e61f019c5847b530977b41740e9b6d90a6e156_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_4ebfbc954e8eaff94ba23fa849e61f019c5847b530977b41740e9b6d90a6e156_d.txt?sv=2019-02-02&sr=b&sig=QRoS7ZUjLiJVSmX6dOdMMJOASk7Pq92AfKcur%2FNcNTE%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_686488706d47327f446523a12bf763bcaf3a1e62c0e187f4f9acd742aa87b9b7_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_686488706d47327f446523a12bf763bcaf3a1e62c0e187f4f9acd742aa87b9b7_d.txt?sv=2019-02-02&sr=b&sig=%2FuEm1QbMKyErs%2Fmd7%2BVJfyDa0Lg2S92qf3b%2BaMWm%2BjM%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_826a6798ef1343f6de0522ca45cd4258e673e6aaa71b49203c946c0dcfeaf412_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_826a6798ef1343f6de0522ca45cd4258e673e6aaa71b49203c946c0dcfeaf412_d.txt?sv=2019-02-02&sr=b&sig=ewo6bXQ3IQmlJjxW0cgONhq2%2BVw7R3DolBGAlL7VxJ0%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_828c38b94e4de744f4b6c387204961ca230972014305f71200f69ffb0b0e8fc5_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_828c38b94e4de744f4b6c387204961ca230972014305f71200f69ffb0b0e8fc5_d.txt?sv=2019-02-02&sr=b&sig=Sa6CWIaHRZLWECPBoeCgySZnp9eYZnIrE0orCXf9drY%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_85c2381e9eb738f8192dedadc86a04bf1f51d042af2f2013a324cf6e6ae0cb98_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_85c2381e9eb738f8192dedadc86a04bf1f51d042af2f2013a324cf6e6ae0cb98_d.txt?sv=2019-02-02&sr=b&sig=0IYtyaQ9M8Xg6V1DFmycrRds1fKzFN5RH2uTIzr6LVo%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_87dac81e6ad142a2bc5557cce1c33abfd1943195c3779a5d27157ad2ab1aae79_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_87dac81e6ad142a2bc5557cce1c33abfd1943195c3779a5d27157ad2ab1aae79_d.txt?sv=2019-02-02&sr=b&sig=vtz7y3IwK0s2LAR68o%2FmibZnSS%2BnY8Yv16sqE1vGkPo%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_89576bae749e54532da0156d97c5b1ea6d51a5e3120f896c36d2f8dd454cb62f_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_89576bae749e54532da0156d97c5b1ea6d51a5e3120f896c36d2f8dd454cb62f_d.txt?sv=2019-02-02&sr=b&sig=o9egYhtFDG5MUAJzERjyfzg9eyvcx%2BNbHDhoGyCfD5o%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_8a51eddec5a1170d0dc4a2bc24e3695f94cf29df4fa16e5a6c3376443a330927_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_8a51eddec5a1170d0dc4a2bc24e3695f94cf29df4fa16e5a6c3376443a330927_d.txt?sv=2019-02-02&sr=b&sig=VqWV8Rg0vSKsjaTZIQga%2BMg%2BVsMT%2Fz7vniFr7rH%2FF%2Fw%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_a30fcf57e2a4ea1ec12c6b65c235418cc8440e6303ea416780b4236e2f35fa71_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_a30fcf57e2a4ea1ec12c6b65c235418cc8440e6303ea416780b4236e2f35fa71_d.txt?sv=2019-02-02&sr=b&sig=S6z5Ed90cjMDoFnoIn9LrtZnR22DGBWYzolZFd9Q9oA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_a74e1461bb5e4a3873ffe490efbd9b3a59cda266d69f77dd43c5b439b18b7f7c_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_a74e1461bb5e4a3873ffe490efbd9b3a59cda266d69f77dd43c5b439b18b7f7c_d.txt?sv=2019-02-02&sr=b&sig=RSPt3kQ86jBoUkkOJo8L%2B3Zy3JrzkagJh9VsY%2BhIZI0%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_c1fb7a092e0221d5758030672ebbc4ac5095c470cd748e5213cce6e7439eb986_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_c1fb7a092e0221d5758030672ebbc4ac5095c470cd748e5213cce6e7439eb986_d.txt?sv=2019-02-02&sr=b&sig=%2BXOS%2F7uYbApSy3Ts2PnOVzm8bon4vIQz40chJ%2FGVldo%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_d6ae0a7bf1e96ef7a9140894146789e6e816418663b5aaf118da42590576efae_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_d6ae0a7bf1e96ef7a9140894146789e6e816418663b5aaf118da42590576efae_d.txt?sv=2019-02-02&sr=b&sig=JHw%2FpehHUfpm%2FKEgy48gcMsKQdUrnjOAO28oy9sesqE%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_e15a45094e869304e1427a640f8c367f4cce89d45ee09b4b9b58608e9bce1707_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_e15a45094e869304e1427a640f8c367f4cce89d45ee09b4b9b58608e9bce1707_d.txt?sv=2019-02-02&sr=b&sig=742CB5SUV0BAqwY%2BH78ZGJbD%2BhRTNpp5iix8r%2BCTOoI%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_e34df470cb61d0e0ffe9bb07a8704dadaa338d2edf86ff5434dc4a9f0328fd52_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_e34df470cb61d0e0ffe9bb07a8704dadaa338d2edf86ff5434dc4a9f0328fd52_d.txt?sv=2019-02-02&sr=b&sig=mqiobMHyTFw3sJVs3a%2FGC6cjcKEUqV3J%2B6nWLGsxWLc%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_f5e6895f19d6bb7262575ac0d051bcb02c32cbba5fc69aebfc0ed545d979128b_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/55_azureml-execution-tvmps_f5e6895f19d6bb7262575ac0d051bcb02c32cbba5fc69aebfc0ed545d979128b_d.txt?sv=2019-02-02&sr=b&sig=PLnIRhZhIIzZIbvujvRXjPqu1ewHmvh2fAVfUcFfQME%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_1155e9e8bc949f0297c44006b498cf5ff82ce349264014bda0202366ac362c74_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_1155e9e8bc949f0297c44006b498cf5ff82ce349264014bda0202366ac362c74_d.txt?sv=2019-02-02&sr=b&sig=tX0dLybJvggHAaUqc7aRcXdZROa9loDLx0GSzYy%2BmBA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_1363d3859a073c8f8a5fe50c10e6f3ab15eefb00d666eec81a7228197d9301d5_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_1363d3859a073c8f8a5fe50c10e6f3ab15eefb00d666eec81a7228197d9301d5_d.txt?sv=2019-02-02&sr=b&sig=pqhCiHgABtwvB1Rd8W5HhF2ekvdls%2Bmiiz0qNlxHmjc%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_1a6015ca0a64f0c7b2abcd18c6bf2d39bfbc9b16cc335339e67ea1450c48db33_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_1a6015ca0a64f0c7b2abcd18c6bf2d39bfbc9b16cc335339e67ea1450c48db33_d.txt?sv=2019-02-02&sr=b&sig=BhEZQmyZTg3%2F2eZxpE1XBhTP4FBczkDaC0oqgisKG80%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_24bcd2b885bc2b7a26754b709fc114f686b931cbe3c451d8fd0593cdd8140387_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_24bcd2b885bc2b7a26754b709fc114f686b931cbe3c451d8fd0593cdd8140387_d.txt?sv=2019-02-02&sr=b&sig=rRNuO8vMfsqE2BinmNqzHxvuwbe81DDdHmjyu74HvjY%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_2757bb30485291f418effccba9e9df670bcb59ff502c6ab3e142272c38ff57e9_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_2757bb30485291f418effccba9e9df670bcb59ff502c6ab3e142272c38ff57e9_d.txt?sv=2019-02-02&sr=b&sig=QSsxsPB9YlVbVwanzPCgIO29c9B%2FdJpNQbsGhp9e%2BAk%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_35fe8ed0e816509418f78d86511693a538092c76ffdf596b83fa9dbcff82167f_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_35fe8ed0e816509418f78d86511693a538092c76ffdf596b83fa9dbcff82167f_d.txt?sv=2019-02-02&sr=b&sig=d0sKcmBZqYiT3R6XapCJzuVzY4QvGzFbSVLrOLZ%2Fn6s%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_43e587ee14b9737fb94fd98b02b62e3cc2855f07e88e87919e134c51d28419cd_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_43e587ee14b9737fb94fd98b02b62e3cc2855f07e88e87919e134c51d28419cd_d.txt?sv=2019-02-02&sr=b&sig=9WFzTJ%2B61g2bmsdE%2F1F1ajmpwOpcKfBUnZWrb%2BJEnEQ%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_489cb80989e58208c697d96047c425f978551633da643ea4b5cfef3acf48925e_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_489cb80989e58208c697d96047c425f978551633da643ea4b5cfef3acf48925e_d.txt?sv=2019-02-02&sr=b&sig=74EBY5zLw1HrXyBV1g%2B%2B7IvcgA9jI0MJeFuwE%2Fsxk%2Bk%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_49fa45ca3caf35eff39803c1aeee2b7f459f281dcc55616fdadb09274161fcae_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_49fa45ca3caf35eff39803c1aeee2b7f459f281dcc55616fdadb09274161fcae_d.txt?sv=2019-02-02&sr=b&sig=VnbG32kVGYvzG8y0MuoyZYXgqXFPFp%2BKGwzA0qe7BE0%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_4ac5e638ef79cd2db7f02922281f1b82fbcc61a6a6c28dcb7d482025ce5642c9_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_4ac5e638ef79cd2db7f02922281f1b82fbcc61a6a6c28dcb7d482025ce5642c9_d.txt?sv=2019-02-02&sr=b&sig=2ppEdSFmstu0kNb3sMn37OarwN6ehSTpgTbPThVYJPw%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_4ebfbc954e8eaff94ba23fa849e61f019c5847b530977b41740e9b6d90a6e156_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_4ebfbc954e8eaff94ba23fa849e61f019c5847b530977b41740e9b6d90a6e156_d.txt?sv=2019-02-02&sr=b&sig=VlOdF3q0XkfAUiVHR3xYd%2Fx%2FSHUByf1dPVcTTbWU8mg%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_686488706d47327f446523a12bf763bcaf3a1e62c0e187f4f9acd742aa87b9b7_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_686488706d47327f446523a12bf763bcaf3a1e62c0e187f4f9acd742aa87b9b7_d.txt?sv=2019-02-02&sr=b&sig=KrO0si4u5LFVRz8EeAlBnXFjqSKumbtBzh5EkoLA2PU%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_826a6798ef1343f6de0522ca45cd4258e673e6aaa71b49203c946c0dcfeaf412_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_826a6798ef1343f6de0522ca45cd4258e673e6aaa71b49203c946c0dcfeaf412_d.txt?sv=2019-02-02&sr=b&sig=LTSRdUQeeqIOAu5Uocl76gTreRbLMw3xRkTbSkL%2BEn8%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_828c38b94e4de744f4b6c387204961ca230972014305f71200f69ffb0b0e8fc5_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_828c38b94e4de744f4b6c387204961ca230972014305f71200f69ffb0b0e8fc5_d.txt?sv=2019-02-02&sr=b&sig=rSv90YoDj5gsBoQV3TGQLA9q0kVjT9znaGnYrsgknms%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_85c2381e9eb738f8192dedadc86a04bf1f51d042af2f2013a324cf6e6ae0cb98_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_85c2381e9eb738f8192dedadc86a04bf1f51d042af2f2013a324cf6e6ae0cb98_d.txt?sv=2019-02-02&sr=b&sig=I%2BhxyK5zy7DIRj7qyUh48lOQzVLOIalCypkKYu1Xhj4%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_87dac81e6ad142a2bc5557cce1c33abfd1943195c3779a5d27157ad2ab1aae79_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_87dac81e6ad142a2bc5557cce1c33abfd1943195c3779a5d27157ad2ab1aae79_d.txt?sv=2019-02-02&sr=b&sig=v2AYOtqeSSCo9GTZ%2BZYhr10UJzq8z6LeM6%2FzIXccdVM%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_89576bae749e54532da0156d97c5b1ea6d51a5e3120f896c36d2f8dd454cb62f_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_89576bae749e54532da0156d97c5b1ea6d51a5e3120f896c36d2f8dd454cb62f_d.txt?sv=2019-02-02&sr=b&sig=6a%2Bxspnh18caVB5JjVrhc8dTYLDTxUxPnAaqnKnDo94%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_8a51eddec5a1170d0dc4a2bc24e3695f94cf29df4fa16e5a6c3376443a330927_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_8a51eddec5a1170d0dc4a2bc24e3695f94cf29df4fa16e5a6c3376443a330927_d.txt?sv=2019-02-02&sr=b&sig=jt14rTFW5cQLsghjeqEzqaVIcJG%2FMD0JigCfwvJWHZo%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_a30fcf57e2a4ea1ec12c6b65c235418cc8440e6303ea416780b4236e2f35fa71_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_a30fcf57e2a4ea1ec12c6b65c235418cc8440e6303ea416780b4236e2f35fa71_d.txt?sv=2019-02-02&sr=b&sig=X55QFQOfAso%2FCbO2U8BGiPdoyYEhzl9%2BXfGZCQ30LUA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_a74e1461bb5e4a3873ffe490efbd9b3a59cda266d69f77dd43c5b439b18b7f7c_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_a74e1461bb5e4a3873ffe490efbd9b3a59cda266d69f77dd43c5b439b18b7f7c_d.txt?sv=2019-02-02&sr=b&sig=JP47cy0FuFgfwde2qnyp6%2Fmilf9832bBxE1DgfbwBas%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_c1fb7a092e0221d5758030672ebbc4ac5095c470cd748e5213cce6e7439eb986_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_c1fb7a092e0221d5758030672ebbc4ac5095c470cd748e5213cce6e7439eb986_d.txt?sv=2019-02-02&sr=b&sig=FyKOJqQBUuTZ3ErOphEz0kpS1Y3gJXEJbE47HPC2%2FVo%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_d6ae0a7bf1e96ef7a9140894146789e6e816418663b5aaf118da42590576efae_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_d6ae0a7bf1e96ef7a9140894146789e6e816418663b5aaf118da42590576efae_d.txt?sv=2019-02-02&sr=b&sig=ouRorN2tttuHZKc3yVJ4wW2evwp5t%2FxnUcc9T%2B2hzkA%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_e15a45094e869304e1427a640f8c367f4cce89d45ee09b4b9b58608e9bce1707_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_e15a45094e869304e1427a640f8c367f4cce89d45ee09b4b9b58608e9bce1707_d.txt?sv=2019-02-02&sr=b&sig=qv%2Fj6NmiA2ltuaGnHIWR3VhasPDS8ixbSTz54tynAGw%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_e34df470cb61d0e0ffe9bb07a8704dadaa338d2edf86ff5434dc4a9f0328fd52_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_e34df470cb61d0e0ffe9bb07a8704dadaa338d2edf86ff5434dc4a9f0328fd52_d.txt?sv=2019-02-02&sr=b&sig=EpLfxo9vjfmbYOgSQPxJE6lH03VkLRM2eXXwydI%2FYnU%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_f5e6895f19d6bb7262575ac0d051bcb02c32cbba5fc69aebfc0ed545d979128b_d.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/65_job_prep-tvmps_f5e6895f19d6bb7262575ac0d051bcb02c32cbba5fc69aebfc0ed545d979128b_d.txt?sv=2019-02-02&sr=b&sig=o7FpfbBj7khf42BPPht3%2FkHaP6fRrQgdDCbtBWkqwrY%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_0.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_0.txt?sv=2019-02-02&sr=b&sig=iCob6Knh8NY2q0gPSYOuGrYPHNH2bjWI6kkpZ5Tz3pI%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_1.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_1.txt?sv=2019-02-02&sr=b&sig=C7nXKFu0ndfBS1Vx4YokG38vLdwN7MJOysXKw9f3VGo%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_10.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_10.txt?sv=2019-02-02&sr=b&sig=D8RJGL0KGPQUQnpw5We0rUMwN4e6mGqCBD%2F%2BnGd8fjA%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_11.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_11.txt?sv=2019-02-02&sr=b&sig=YvA6JdEJkFI0kG0XCM22NWGE1ZbNrGpaal25GdcGB00%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_12.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_12.txt?sv=2019-02-02&sr=b&sig=Hz45Ajhe9bhE%2B2wRAvnuKcv%2BEkYK8pLiwaET%2B6mQWNk%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_13.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_13.txt?sv=2019-02-02&sr=b&sig=7sWLbF2LqAyRl9uNrbwdf%2B4rAMRHcDxzrFj%2BAWlreXo%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_14.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_14.txt?sv=2019-02-02&sr=b&sig=zLqyuff0dmZqvz79U5xRy14TbwNANv2NnLPnjjisfRo%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_15.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_15.txt?sv=2019-02-02&sr=b&sig=HfLdlECmIlMLNjnMLOyxNd%2BAOhV5tLTCXIaWLYM07AI%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_16.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_16.txt?sv=2019-02-02&sr=b&sig=SCQWox%2BdFIZIhjqd44xaiYfQPFKLeoOJrfMJlnK1ipw%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_17.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_17.txt?sv=2019-02-02&sr=b&sig=a%2Ba91X%2FhyerOepQIbe2v129hGNrP5y4%2FoVFMjWny54Y%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_18.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_18.txt?sv=2019-02-02&sr=b&sig=6z9hNrP3r1kn0kR0P2QqxwzfXaiEjnHHKc4RVvhTkFw%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_19.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_19.txt?sv=2019-02-02&sr=b&sig=lBkxQOF%2FhDG%2Fenv4YdAAf9RicB02zB4%2BPKGpEEdiFc4%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_2.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_2.txt?sv=2019-02-02&sr=b&sig=K0C%2FCTO4yuKdb4KFZINq96iWfJUYVkoNAUrhHWvO0NI%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_20.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_20.txt?sv=2019-02-02&sr=b&sig=4puwSFC6CrIMGU1e6d%2BpyPoK9CjyPdG5XfWSAVxIlaI%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_21.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_21.txt?sv=2019-02-02&sr=b&sig=VB9mpP5eUh1cSi8XpBdBScvAJSbfQrZlnO0h92%2FACts%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_22.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_22.txt?sv=2019-02-02&sr=b&sig=SIOUa9kwRpJNOp0gYZBcLwSCYEtFT%2FYq07XIABym8H0%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_23.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_23.txt?sv=2019-02-02&sr=b&sig=v4D5BmrMSshx%2BmPMIw7kg1uSnPO2qd6FfrvSvfrIj0s%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_24.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_24.txt?sv=2019-02-02&sr=b&sig=OfDqx5qRcmLGtIRJzzgMypcBcijXGSfvN7OmgZdYjTg%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_3.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_3.txt?sv=2019-02-02&sr=b&sig=Z80MOy8TRQeH8JnPwdW4%2FCK2qlIk%2B22h31H2Zzk48Y0%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_4.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_4.txt?sv=2019-02-02&sr=b&sig=LgYJNAx1koQsoNqjWw1dy%2FFAKjthsHTQ6QYlJzEZhnk%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_5.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_5.txt?sv=2019-02-02&sr=b&sig=1jrDWHlSyV4kulCwfevhupSxdZ%2Fku0aMoqoOBG1aBmk%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_6.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_6.txt?sv=2019-02-02&sr=b&sig=w81HuwQ6Un7ZD9x5Nau647BUZ9BE9g%2F1kJQgMhLSW30%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_7.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_7.txt?sv=2019-02-02&sr=b&sig=DZ2d5kLSf3xq8gIlxdq7gb76v0ILa0d1hqRriEu1jD8%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_8.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_8.txt?sv=2019-02-02&sr=b&sig=i3AwIL4TrB8qPFbOU1qvDLiTl3zsgZsOPQUgognDk00%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_driver_log_9.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_driver_log_9.txt?sv=2019-02-02&sr=b&sig=ggiRGkeYOwDQmfbh%2BEZU0YhBntacivhaVhy4CDjo%2F%2B4%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/70_mpi_log.txt\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/70_mpi_log.txt?sv=2019-02-02&sr=b&sig=fGYP5EAHGp6Ym2p728gF4HfhtzOVOiTC8PhVYNMPxpE%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/process_info.json\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=cbc%2B9S58OeAPnLB79NgiFvxdi1bb44oklkvHtNpqz3E%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"azureml-logs/process_status.json\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=h9VDTlnXGBIbdtx4SkwrQbQFxXGWNIyBVdoS5ns3fu4%3D&st=2020-01-31T04%3A21%3A14Z&se=2020-01-31T12%3A31%3A14Z&sp=r\", \"logs/azureml/0_201_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/0_201_azureml.log?sv=2019-02-02&sr=b&sig=Cg4ut4%2FcI4jB8IO7vUf54ZxQ1nEoUEvNXJ%2BW33TaKUg%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/10_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/10_135_azureml.log?sv=2019-02-02&sr=b&sig=itvLVD5pb2EwSWcBM9kvBeoi9SkLC91qBO9Z0dfPRCA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/11_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/11_134_azureml.log?sv=2019-02-02&sr=b&sig=SYH8stmMPhQpU3MN4AFfVIAJIqtJum9vtW0iYi9PbRM%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/12_136_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/12_136_azureml.log?sv=2019-02-02&sr=b&sig=vlRk9xKvaTOqVCh1oAKY%2Ba9MhnZK7zfWAJ8t1s%2Fbbx0%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/13_136_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/13_136_azureml.log?sv=2019-02-02&sr=b&sig=YeBLwtMl7%2Fah8qgB%2BBnEb4kLlzvVVxxOsu6WCvaQpxo%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/14_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/14_134_azureml.log?sv=2019-02-02&sr=b&sig=EXW%2BQ%2FHjqvhq3szuKJ1nTitZamZS89DLQyn8UMefRCw%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/15_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/15_134_azureml.log?sv=2019-02-02&sr=b&sig=u%2BZUB7ClGfPNIe37wE81Sdk9dRp7driWqBn7Yo2loLA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/16_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/16_134_azureml.log?sv=2019-02-02&sr=b&sig=qqAgw5NHvHAqG%2FUpLOivYxGXfqJGc3YsPuHRrCDQwoc%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/17_133_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/17_133_azureml.log?sv=2019-02-02&sr=b&sig=rh64x8otFQqTx7%2FfpLx%2FkHc9NRPgfnigqZ06Bi%2FjpZw%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/18_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/18_135_azureml.log?sv=2019-02-02&sr=b&sig=K8NLAStbbe%2Fs%2Fj6UgTwZHiTnmF6NZOrIYbjmmMkNLTU%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/19_136_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/19_136_azureml.log?sv=2019-02-02&sr=b&sig=BTcGXGuA8OGWGkMgZVjLgsNgwb4KkzMW%2B7zs3Io9txo%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/1_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/1_135_azureml.log?sv=2019-02-02&sr=b&sig=4vTfYnEnYfM8DNOZiv%2FlQcNrCzjlNFSL%2BLCeiHLrfiA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/20_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/20_134_azureml.log?sv=2019-02-02&sr=b&sig=ikMQpjzlF7Ofye9x4ccnWYFX74V92gXehHf5krrzJcc%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/21_133_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/21_133_azureml.log?sv=2019-02-02&sr=b&sig=8e4rdl5EiSYrmzg4nUpxaKHpOVUmQKIto5kedODbaO4%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/22_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/22_135_azureml.log?sv=2019-02-02&sr=b&sig=qS2EjNwXV9In1PKvjUuGwj7E5mN93SUzCe0nzj9GMIg%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/23_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/23_135_azureml.log?sv=2019-02-02&sr=b&sig=xfIWdhqBLLM7r0L51%2F7SRBsByr%2FQJ87vUqpFFsRlw0w%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/24_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/24_135_azureml.log?sv=2019-02-02&sr=b&sig=pHSWT2i7tuQpqqT6k2UT%2FhHXlzRtcn7BGmlRk7s5KOk%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/2_136_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/2_136_azureml.log?sv=2019-02-02&sr=b&sig=80oE0PNzr7v0SEJv14xOer7jqMruMU0egBRW5cT%2FJ8g%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/3_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/3_135_azureml.log?sv=2019-02-02&sr=b&sig=7soCbmBjVgGL5ZjvDIx%2Bgh5A5Xa4WYnq0FMRNic1Y9g%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/4_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/4_134_azureml.log?sv=2019-02-02&sr=b&sig=vz7T43DT4Kqm0MtCUFnCPowq4Qd%2B8A94QlIB%2B%2FKNmxA%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/5_133_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/5_133_azureml.log?sv=2019-02-02&sr=b&sig=Iv257ev7AWo2zYip%2Bkpht3KGRWMTRf9eF0Vrx5bIuvw%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/6_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/6_134_azureml.log?sv=2019-02-02&sr=b&sig=CrfXtdLMRAwlMOFWKOGF0H5PuoZcO01H4Mte5iUPmzM%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/7_134_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/7_134_azureml.log?sv=2019-02-02&sr=b&sig=Q65vhmUfUE2FD4Z%2B0Y6SWE1Rcw06jRL1LHiBPblBB4M%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/8_133_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/8_133_azureml.log?sv=2019-02-02&sr=b&sig=DmpQNGmQsO0trJXE3GB%2F9O%2BdQ7lOKytg%2Fj48ivRVXxU%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/9_135_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/9_135_azureml.log?sv=2019-02-02&sr=b&sig=HOiph67DTqbKIMIK7GO17Pmqy0dnKY3MMIa4pAsMxeE%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://azuremldask4012756815.blob.core.windows.net/azureml/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=PyF2I7Vsedq1Bg6U%2FKfohDzz7pEvcz9AEgQ0R%2B68Cvk%3D&st=2020-01-31T04%3A21%3A13Z&se=2020-01-31T12%3A31%3A13Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\"], [\"logs/azureml/0_201_azureml.log\"], [\"logs/azureml/1_135_azureml.log\"], [\"logs/azureml/2_136_azureml.log\"], [\"logs/azureml/3_135_azureml.log\"], [\"logs/azureml/4_134_azureml.log\"], [\"logs/azureml/5_133_azureml.log\"], [\"logs/azureml/6_134_azureml.log\"], [\"logs/azureml/7_134_azureml.log\"], [\"logs/azureml/8_133_azureml.log\"], [\"logs/azureml/9_135_azureml.log\"], [\"logs/azureml/10_135_azureml.log\"], [\"logs/azureml/11_134_azureml.log\"], [\"logs/azureml/12_136_azureml.log\"], [\"logs/azureml/13_136_azureml.log\"], [\"logs/azureml/14_134_azureml.log\"], [\"logs/azureml/15_134_azureml.log\"], [\"logs/azureml/16_134_azureml.log\"], [\"logs/azureml/17_133_azureml.log\"], [\"logs/azureml/18_135_azureml.log\"], [\"logs/azureml/19_136_azureml.log\"], [\"logs/azureml/20_134_azureml.log\"], [\"logs/azureml/21_133_azureml.log\"], [\"logs/azureml/22_135_azureml.log\"], [\"logs/azureml/23_135_azureml.log\"], [\"logs/azureml/24_135_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_c1fb7a092e0221d5758030672ebbc4ac5095c470cd748e5213cce6e7439eb986_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_1a6015ca0a64f0c7b2abcd18c6bf2d39bfbc9b16cc335339e67ea1450c48db33_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_4ac5e638ef79cd2db7f02922281f1b82fbcc61a6a6c28dcb7d482025ce5642c9_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_4ebfbc954e8eaff94ba23fa849e61f019c5847b530977b41740e9b6d90a6e156_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_f5e6895f19d6bb7262575ac0d051bcb02c32cbba5fc69aebfc0ed545d979128b_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_d6ae0a7bf1e96ef7a9140894146789e6e816418663b5aaf118da42590576efae_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_8a51eddec5a1170d0dc4a2bc24e3695f94cf29df4fa16e5a6c3376443a330927_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_e15a45094e869304e1427a640f8c367f4cce89d45ee09b4b9b58608e9bce1707_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_24bcd2b885bc2b7a26754b709fc114f686b931cbe3c451d8fd0593cdd8140387_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_a30fcf57e2a4ea1ec12c6b65c235418cc8440e6303ea416780b4236e2f35fa71_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_e34df470cb61d0e0ffe9bb07a8704dadaa338d2edf86ff5434dc4a9f0328fd52_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_35fe8ed0e816509418f78d86511693a538092c76ffdf596b83fa9dbcff82167f_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_43e587ee14b9737fb94fd98b02b62e3cc2855f07e88e87919e134c51d28419cd_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_49fa45ca3caf35eff39803c1aeee2b7f459f281dcc55616fdadb09274161fcae_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_a74e1461bb5e4a3873ffe490efbd9b3a59cda266d69f77dd43c5b439b18b7f7c_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_85c2381e9eb738f8192dedadc86a04bf1f51d042af2f2013a324cf6e6ae0cb98_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_87dac81e6ad142a2bc5557cce1c33abfd1943195c3779a5d27157ad2ab1aae79_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_489cb80989e58208c697d96047c425f978551633da643ea4b5cfef3acf48925e_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_826a6798ef1343f6de0522ca45cd4258e673e6aaa71b49203c946c0dcfeaf412_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_828c38b94e4de744f4b6c387204961ca230972014305f71200f69ffb0b0e8fc5_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_1155e9e8bc949f0297c44006b498cf5ff82ce349264014bda0202366ac362c74_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_1363d3859a073c8f8a5fe50c10e6f3ab15eefb00d666eec81a7228197d9301d5_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_2757bb30485291f418effccba9e9df670bcb59ff502c6ab3e142272c38ff57e9_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_89576bae749e54532da0156d97c5b1ea6d51a5e3120f896c36d2f8dd454cb62f_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_686488706d47327f446523a12bf763bcaf3a1e62c0e187f4f9acd742aa87b9b7_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_c1fb7a092e0221d5758030672ebbc4ac5095c470cd748e5213cce6e7439eb986_d.txt\", \"azureml-logs/65_job_prep-tvmps_1a6015ca0a64f0c7b2abcd18c6bf2d39bfbc9b16cc335339e67ea1450c48db33_d.txt\", \"azureml-logs/65_job_prep-tvmps_4ac5e638ef79cd2db7f02922281f1b82fbcc61a6a6c28dcb7d482025ce5642c9_d.txt\", \"azureml-logs/65_job_prep-tvmps_4ebfbc954e8eaff94ba23fa849e61f019c5847b530977b41740e9b6d90a6e156_d.txt\", \"azureml-logs/65_job_prep-tvmps_f5e6895f19d6bb7262575ac0d051bcb02c32cbba5fc69aebfc0ed545d979128b_d.txt\", \"azureml-logs/65_job_prep-tvmps_d6ae0a7bf1e96ef7a9140894146789e6e816418663b5aaf118da42590576efae_d.txt\", \"azureml-logs/65_job_prep-tvmps_8a51eddec5a1170d0dc4a2bc24e3695f94cf29df4fa16e5a6c3376443a330927_d.txt\", \"azureml-logs/65_job_prep-tvmps_e15a45094e869304e1427a640f8c367f4cce89d45ee09b4b9b58608e9bce1707_d.txt\", \"azureml-logs/65_job_prep-tvmps_24bcd2b885bc2b7a26754b709fc114f686b931cbe3c451d8fd0593cdd8140387_d.txt\", \"azureml-logs/65_job_prep-tvmps_a30fcf57e2a4ea1ec12c6b65c235418cc8440e6303ea416780b4236e2f35fa71_d.txt\", \"azureml-logs/65_job_prep-tvmps_e34df470cb61d0e0ffe9bb07a8704dadaa338d2edf86ff5434dc4a9f0328fd52_d.txt\", \"azureml-logs/65_job_prep-tvmps_35fe8ed0e816509418f78d86511693a538092c76ffdf596b83fa9dbcff82167f_d.txt\", \"azureml-logs/65_job_prep-tvmps_43e587ee14b9737fb94fd98b02b62e3cc2855f07e88e87919e134c51d28419cd_d.txt\", \"azureml-logs/65_job_prep-tvmps_49fa45ca3caf35eff39803c1aeee2b7f459f281dcc55616fdadb09274161fcae_d.txt\", \"azureml-logs/65_job_prep-tvmps_a74e1461bb5e4a3873ffe490efbd9b3a59cda266d69f77dd43c5b439b18b7f7c_d.txt\", \"azureml-logs/65_job_prep-tvmps_85c2381e9eb738f8192dedadc86a04bf1f51d042af2f2013a324cf6e6ae0cb98_d.txt\", \"azureml-logs/65_job_prep-tvmps_87dac81e6ad142a2bc5557cce1c33abfd1943195c3779a5d27157ad2ab1aae79_d.txt\", \"azureml-logs/65_job_prep-tvmps_489cb80989e58208c697d96047c425f978551633da643ea4b5cfef3acf48925e_d.txt\", \"azureml-logs/65_job_prep-tvmps_826a6798ef1343f6de0522ca45cd4258e673e6aaa71b49203c946c0dcfeaf412_d.txt\", \"azureml-logs/65_job_prep-tvmps_828c38b94e4de744f4b6c387204961ca230972014305f71200f69ffb0b0e8fc5_d.txt\", \"azureml-logs/65_job_prep-tvmps_1155e9e8bc949f0297c44006b498cf5ff82ce349264014bda0202366ac362c74_d.txt\", \"azureml-logs/65_job_prep-tvmps_1363d3859a073c8f8a5fe50c10e6f3ab15eefb00d666eec81a7228197d9301d5_d.txt\", \"azureml-logs/65_job_prep-tvmps_2757bb30485291f418effccba9e9df670bcb59ff502c6ab3e142272c38ff57e9_d.txt\", \"azureml-logs/65_job_prep-tvmps_89576bae749e54532da0156d97c5b1ea6d51a5e3120f896c36d2f8dd454cb62f_d.txt\", \"azureml-logs/65_job_prep-tvmps_686488706d47327f446523a12bf763bcaf3a1e62c0e187f4f9acd742aa87b9b7_d.txt\"], [\"azureml-logs/70_mpi_log.txt\", \"azureml-logs/70_driver_log_0.txt\", \"azureml-logs/70_driver_log_1.txt\", \"azureml-logs/70_driver_log_2.txt\", \"azureml-logs/70_driver_log_3.txt\", \"azureml-logs/70_driver_log_4.txt\", \"azureml-logs/70_driver_log_5.txt\", \"azureml-logs/70_driver_log_6.txt\", \"azureml-logs/70_driver_log_7.txt\", \"azureml-logs/70_driver_log_8.txt\", \"azureml-logs/70_driver_log_9.txt\", \"azureml-logs/70_driver_log_10.txt\", \"azureml-logs/70_driver_log_11.txt\", \"azureml-logs/70_driver_log_12.txt\", \"azureml-logs/70_driver_log_13.txt\", \"azureml-logs/70_driver_log_14.txt\", \"azureml-logs/70_driver_log_15.txt\", \"azureml-logs/70_driver_log_16.txt\", \"azureml-logs/70_driver_log_17.txt\", \"azureml-logs/70_driver_log_18.txt\", \"azureml-logs/70_driver_log_19.txt\", \"azureml-logs/70_driver_log_20.txt\", \"azureml-logs/70_driver_log_21.txt\", \"azureml-logs/70_driver_log_22.txt\", \"azureml-logs/70_driver_log_23.txt\", \"azureml-logs/70_driver_log_24.txt\"]], \"run_duration\": \"1:13:42\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"nodes\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [25]}]}, {\"name\": \"headnode\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"10.3.0.5\"]}]}, {\"name\": \"scheduler\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"10.3.0.5:8786\"]}]}, {\"name\": \"dashboard\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"10.3.0.5:8787\"]}]}, {\"name\": \"jupyter\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"10.3.0.5:8888\"]}]}, {\"name\": \"token\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"988129fa43d811eaa13f000d3ad498da\"]}]}, {\"name\": \"codestore\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/codefileshare\"]}]}, {\"name\": \"datastore\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/gen2\"]}]}, {\"name\": \"mean_latitude\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_latitude_1580442171.png\"]}]}, {\"name\": \"mean_longitude\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_longitude_1580442172.png\"]}]}, {\"name\": \"mean_elevation\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_elevation_1580442172.png\"]}]}, {\"name\": \"mean_windAngle\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_windAngle_1580442173.png\"]}]}, {\"name\": \"mean_windSpeed\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_windSpeed_1580442173.png\"]}]}, {\"name\": \"mean_temperature\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_temperature_1580442174.png\"]}]}, {\"name\": \"mean_seaLvlPressure\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_seaLvlPressure_1580442174.png\"]}]}, {\"name\": \"mean_presentWeatherIndicator\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_presentWeatherIndicator_1580442175.png\"]}]}, {\"name\": \"mean_pastWeatherIndicator\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_pastWeatherIndicator_1580442175.png\"]}]}, {\"name\": \"mean_precipTime\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_precipTime_1580442176.png\"]}]}, {\"name\": \"mean_precipDepth\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_precipDepth_1580442176.png\"]}]}, {\"name\": \"mean_snowDepth\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_snowDepth_1580442177.png\"]}]}, {\"name\": \"mean_year\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_year_1580442177.png\"]}]}, {\"name\": \"mean_day\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_day_1580442178.png\"]}]}, {\"name\": \"mean_version\", \"run_id\": \"dask-dask-demo_1580440649_506153b5\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.dask-dask-demo_1580440649_506153b5/mean_version_1580442178.png\"]}]}], \"run_logs\": \"bash: /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/libtinfo.so.5: no version information available (required by bash)\\nThis is an MPI job. Rank:0\\nStarting the daemon thread to refresh tokens in background for process with pid = 201\\nEntering Run History Context Manager.\\n- scheduler is  10.3.0.5:8786\\n- dashboard is  10.3.0.5:8787\\n- args:  Namespace(code_store='/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/codefileshare', dashboard_port=8787, data_store='/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/gen2', jupyter='true', jupyter_port=8888, jupyter_token='988129fa43d811eaa13f000d3ad498da', n_gpus_per_node=0, scheduler_port=8786, script=None, use_GPU=False)\\n- unparsed:  []\\n- my rank is  0\\n- my ip is  10.3.0.5\\n[]\\n[I 03:20:15.433 LabApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\\n[I 03:20:15.801 LabApp] JupyterLab extension loaded from /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/jupyterlab\\n[I 03:20:15.801 LabApp] JupyterLab application directory is /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/share/jupyter/lab\\n[I 03:20:15.805 LabApp] Serving notebooks from local directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/codefileshare/..\\n[I 03:20:15.805 LabApp] The Jupyter Notebook is running at:\\n[I 03:20:15.805 LabApp] http://bb33630bb27d4d9dadb1b2d760c5d4a7000000:8888/?token=...\\n[I 03:20:15.805 LabApp]  or http://127.0.0.1:8888/?token=...\\n[I 03:20:15.805 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\\ndistributed.scheduler - INFO - -----------------------------------------------\\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.5:40205'\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.scheduler - INFO - Local Directory:    /tmp/scheduler-yv74nl7h\\ndistributed.scheduler - INFO - -----------------------------------------------\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO -   Scheduler at:       tcp://10.3.0.5:8786\\ndistributed.scheduler - INFO -   dashboard at:                     :8787\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.20:45003', name: tcp://10.3.0.20:45003, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.20:45003\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.15:33617', name: tcp://10.3.0.15:33617, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:33617\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.9:40739', name: tcp://10.3.0.9:40739, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.9:40739\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.18:40161', name: tcp://10.3.0.18:40161, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.18:40161\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.7:44013', name: tcp://10.3.0.7:44013, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.7:44013\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.10:44429', name: tcp://10.3.0.10:44429, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:44429\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.6:34959', name: tcp://10.3.0.6:34959, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:34959\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.8:43747', name: tcp://10.3.0.8:43747, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:43747\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.21:34475', name: tcp://10.3.0.21:34475, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.21:34475\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.29:43435', name: tcp://10.3.0.29:43435, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.29:43435\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.17:33099', name: tcp://10.3.0.17:33099, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:33099\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.13:46305', name: tcp://10.3.0.13:46305, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.13:46305\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.12:41015', name: tcp://10.3.0.12:41015, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.12:41015\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.16:36005', name: tcp://10.3.0.16:36005, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.16:36005\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.14:39595', name: tcp://10.3.0.14:39595, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:39595\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.11:38223', name: tcp://10.3.0.11:38223, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.11:38223\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.23:46307', name: tcp://10.3.0.23:46307, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.23:46307\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.30:42175', name: tcp://10.3.0.30:42175, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.30:42175\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.27:34021', name: tcp://10.3.0.27:34021, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.27:34021\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.26:38645', name: tcp://10.3.0.26:38645, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.26:38645\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.25:36733', name: tcp://10.3.0.25:36733, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:36733\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.24:45859', name: tcp://10.3.0.24:45859, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:45859\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.22:44247', name: tcp://10.3.0.22:44247, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:44247\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.19:39749', name: tcp://10.3.0.19:39749, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.19:39749\\ndistributed.core - INFO - Starting established connection\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-0gju49gm', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-2puq321a', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-2t60bpqt', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-32rd7f7r', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-4wsgsjym', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-6pq70bph', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-98r0cjva', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-_nvmtqua', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-eipbo6q0', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-ekdthe7l', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-gqjerevv', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-hfi41csj', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-hs8qyzut', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-kgpomj5i', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-mkuol45r', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-naeb9p1j', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-pnlkjoa1', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-prjol8i1', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-qu9tmm_u', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-r_dxxe88', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-v59pg4mp', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-vdxw_7xj', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-xx_0abmg', purging\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-zkvkipco', purging\\n/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/contextlib.py:88: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\\n  next(self.gen)\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:34875\\ndistributed.worker - INFO -          Listening to:       tcp://10.3.0.5:34875\\ndistributed.worker - INFO -          dashboard at:             10.3.0.5:36713\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                         16\\ndistributed.worker - INFO -                Memory:                  118.27 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-hmyc6cpo\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.5:34875', name: tcp://10.3.0.5:34875, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:34875\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\n[W 03:20:34.648 LabApp] Could not determine jupyterlab build status without nodejs\\n[I 03:20:43.517 LabApp] Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret\\n[W 03:20:43.521 LabApp] Notebook codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb is not trusted\\n[I 03:20:46.058 LabApp] Kernel started: 13227a22-fba6-40f1-a901-07b924594272\\n[I 03:22:19.042 LabApp] Kernel restarted: 13227a22-fba6-40f1-a901-07b924594272\\ndistributed.scheduler - INFO - Receive client connection: Client-e5ef7afe-43d8-11ea-81bf-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\\nCollecting matplotlib\\n  Downloading matplotlib-3.1.2-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\\nCollecting kiwisolver>=1.0.1\\n  Downloading kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90 kB)\\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.18.1)\\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.4.6)\\nCollecting cycler>=0.10\\n  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.8.1)\\nRequirement already satisfied, skipping upgrade: setuptools in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200119)\\nRequirement already satisfied, skipping upgrade: six in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\\nInstalling collected packages: kiwisolver, cycler, matplotlib\\nSuccessfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.2\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.10:44429', name: tcp://10.3.0.10:44429, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.10:44429\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.11:38223', name: tcp://10.3.0.11:38223, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.11:38223\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.12:41015', name: tcp://10.3.0.12:41015, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.12:41015\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.13:46305', name: tcp://10.3.0.13:46305, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.13:46305\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.14:39595', name: tcp://10.3.0.14:39595, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.14:39595\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.15:33617', name: tcp://10.3.0.15:33617, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.15:33617\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.16:36005', name: tcp://10.3.0.16:36005, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.16:36005\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.17:33099', name: tcp://10.3.0.17:33099, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.17:33099\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.18:40161', name: tcp://10.3.0.18:40161, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.18:40161\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.19:39749', name: tcp://10.3.0.19:39749, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.19:39749\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.20:45003', name: tcp://10.3.0.20:45003, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.20:45003\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.21:34475', name: tcp://10.3.0.21:34475, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.21:34475\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.22:44247', name: tcp://10.3.0.22:44247, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.22:44247\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.23:46307', name: tcp://10.3.0.23:46307, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.23:46307\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.24:45859', name: tcp://10.3.0.24:45859, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.24:45859\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.25:36733', name: tcp://10.3.0.25:36733, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.25:36733\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.26:38645', name: tcp://10.3.0.26:38645, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.26:38645\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.27:34021', name: tcp://10.3.0.27:34021, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.27:34021\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.29:43435', name: tcp://10.3.0.29:43435, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.29:43435\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.30:42175', name: tcp://10.3.0.30:42175, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.30:42175\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.5:34875', name: tcp://10.3.0.5:34875, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.5:34875\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.6:34959', name: tcp://10.3.0.6:34959, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.6:34959\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.7:44013', name: tcp://10.3.0.7:44013, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.7:44013\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.8:43747', name: tcp://10.3.0.8:43747, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.8:43747\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.9:40739', name: tcp://10.3.0.9:40739, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.9:40739\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.worker - INFO - Stopping worker at tcp://10.3.0.5:34875\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.6:35501', name: tcp://10.3.0.6:35501, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:35501\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.23:35587', name: tcp://10.3.0.23:35587, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.23:35587\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.13:41013', name: tcp://10.3.0.13:41013, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.13:41013\\ndistributed.core - INFO - Starting established connection\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.7:33897', name: tcp://10.3.0.7:33897, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.7:33897\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.11:37341', name: tcp://10.3.0.11:37341, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.11:37341\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:34939\\ndistributed.worker - INFO -          Listening to:       tcp://10.3.0.5:34939\\ndistributed.worker - INFO -          dashboard at:             10.3.0.5:41891\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                         16\\ndistributed.worker - INFO -                Memory:                  118.27 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-dn4oxa0l\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.16:45953', name: tcp://10.3.0.16:45953, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.16:45953\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.24:44681', name: tcp://10.3.0.24:44681, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:44681\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.14:43501', name: tcp://10.3.0.14:43501, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:43501\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.8:41315', name: tcp://10.3.0.8:41315, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:41315\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.30:32905', name: tcp://10.3.0.30:32905, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.30:32905\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.18:35013', name: tcp://10.3.0.18:35013, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.18:35013\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.20:39229', name: tcp://10.3.0.20:39229, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.20:39229\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.9:43797', name: tcp://10.3.0.9:43797, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.9:43797\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.10:34873', name: tcp://10.3.0.10:34873, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:34873\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.29:32837', name: tcp://10.3.0.29:32837, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.29:32837\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.12:42737', name: tcp://10.3.0.12:42737, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.12:42737\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.5:34939', name: tcp://10.3.0.5:34939, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:34939\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.22:34825', name: tcp://10.3.0.22:34825, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:34825\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.26:34239', name: tcp://10.3.0.26:34239, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.26:34239\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.25:36021', name: tcp://10.3.0.25:36021, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:36021\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.27:39633', name: tcp://10.3.0.27:39633, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.27:39633\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.15:44653', name: tcp://10.3.0.15:44653, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:44653\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.19:39517', name: tcp://10.3.0.19:39517, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.19:39517\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.21:43607', name: tcp://10.3.0.21:43607, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.21:43607\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.17:38241', name: tcp://10.3.0.17:38241, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:38241\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Receive client connection: Client-ed6e3f1a-43d8-11ea-81bf-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\n[I 03:22:45.404 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:24:26.696 LabApp] Kernel interrupted: 13227a22-fba6-40f1-a901-07b924594272\\n[I 03:24:46.249 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:26:46.957 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:26:52.199 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:27:05.776 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\ndistributed.scheduler - INFO - Remove client Client-ed6e3f1a-43d8-11ea-81bf-000d3ad498da\\ndistributed.scheduler - INFO - Remove client Client-ed6e3f1a-43d8-11ea-81bf-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-ed6e3f1a-43d8-11ea-81bf-000d3ad498da\\ndistributed.scheduler - INFO - Remove client Client-e5ef7afe-43d8-11ea-81bf-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-e5ef7afe-43d8-11ea-81bf-000d3ad498da\\n[I 03:27:11.035 LabApp] Kernel restarted: 13227a22-fba6-40f1-a901-07b924594272\\ndistributed.scheduler - INFO - Receive client connection: Client-93613690-43d9-11ea-8338-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\\nRequirement already up-to-date: matplotlib in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (3.1.2)\\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.18.1)\\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.8.1)\\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (0.10.0)\\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.4.6)\\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.1.0)\\nRequirement already satisfied, skipping upgrade: six>=1.5 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\\nRequirement already satisfied, skipping upgrade: setuptools in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200119)\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.10:34873', name: tcp://10.3.0.10:34873, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.10:34873\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.11:37341', name: tcp://10.3.0.11:37341, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.11:37341\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.12:42737', name: tcp://10.3.0.12:42737, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.12:42737\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.13:41013', name: tcp://10.3.0.13:41013, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.13:41013\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.14:43501', name: tcp://10.3.0.14:43501, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.14:43501\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.15:44653', name: tcp://10.3.0.15:44653, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.15:44653\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.16:45953', name: tcp://10.3.0.16:45953, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.16:45953\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.17:38241', name: tcp://10.3.0.17:38241, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.17:38241\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.18:35013', name: tcp://10.3.0.18:35013, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.18:35013\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.19:39517', name: tcp://10.3.0.19:39517, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.19:39517\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.20:39229', name: tcp://10.3.0.20:39229, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.20:39229\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.21:43607', name: tcp://10.3.0.21:43607, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.21:43607\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.22:34825', name: tcp://10.3.0.22:34825, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.22:34825\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.23:35587', name: tcp://10.3.0.23:35587, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.23:35587\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.24:44681', name: tcp://10.3.0.24:44681, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.24:44681\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.25:36021', name: tcp://10.3.0.25:36021, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.25:36021\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.26:34239', name: tcp://10.3.0.26:34239, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.26:34239\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.27:39633', name: tcp://10.3.0.27:39633, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.27:39633\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.29:32837', name: tcp://10.3.0.29:32837, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.29:32837\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.30:32905', name: tcp://10.3.0.30:32905, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.30:32905\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.5:34939', name: tcp://10.3.0.5:34939, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.5:34939\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.6:35501', name: tcp://10.3.0.6:35501, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.6:35501\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.7:33897', name: tcp://10.3.0.7:33897, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.7:33897\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.8:41315', name: tcp://10.3.0.8:41315, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.8:41315\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.9:43797', name: tcp://10.3.0.9:43797, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.9:43797\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.worker - INFO - Stopping worker at tcp://10.3.0.5:34939\\ndistributed.nanny - INFO - Worker closed\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:37827\\ndistributed.worker - INFO -          Listening to:       tcp://10.3.0.5:37827\\ndistributed.worker - INFO -          dashboard at:             10.3.0.5:43141\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                         16\\ndistributed.worker - INFO -                Memory:                  118.27 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-zjma6mzw\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.7:46429', name: tcp://10.3.0.7:46429, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.7:46429\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.27:40517', name: tcp://10.3.0.27:40517, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.27:40517\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.23:41599', name: tcp://10.3.0.23:41599, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.23:41599\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.25:41407', name: tcp://10.3.0.25:41407, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:41407\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.12:33393', name: tcp://10.3.0.12:33393, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.12:33393\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.9:33145', name: tcp://10.3.0.9:33145, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.9:33145\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.8:40649', name: tcp://10.3.0.8:40649, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:40649\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.10:41583', name: tcp://10.3.0.10:41583, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:41583\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.19:37979', name: tcp://10.3.0.19:37979, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.19:37979\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.5:37827', name: tcp://10.3.0.5:37827, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:37827\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.30:36267', name: tcp://10.3.0.30:36267, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.30:36267\\ndistributed.core - INFO - Starting established connection\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.22:37321', name: tcp://10.3.0.22:37321, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:37321\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.29:40539', name: tcp://10.3.0.29:40539, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.29:40539\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.6:37499', name: tcp://10.3.0.6:37499, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:37499\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.18:35841', name: tcp://10.3.0.18:35841, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.18:35841\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.20:35541', name: tcp://10.3.0.20:35541, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.20:35541\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.24:46569', name: tcp://10.3.0.24:46569, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:46569\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.21:45521', name: tcp://10.3.0.21:45521, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.21:45521\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.14:34091', name: tcp://10.3.0.14:34091, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:34091\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.15:41959', name: tcp://10.3.0.15:41959, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:41959\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.16:40787', name: tcp://10.3.0.16:40787, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.16:40787\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.13:32861', name: tcp://10.3.0.13:32861, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.13:32861\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.11:35845', name: tcp://10.3.0.11:35845, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.11:35845\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.26:42475', name: tcp://10.3.0.26:42475, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.26:42475\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.17:35795', name: tcp://10.3.0.17:35795, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:35795\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Receive client connection: Client-9687c99a-43d9-11ea-8338-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\n[I 03:29:06.477 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.scheduler - INFO - Remove client Client-9687c99a-43d9-11ea-8338-000d3ad498da\\ndistributed.scheduler - INFO - Remove client Client-93613690-43d9-11ea-8338-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-9687c99a-43d9-11ea-8338-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-93613690-43d9-11ea-8338-000d3ad498da\\n[I 03:30:01.504 LabApp] Kernel restarted: 13227a22-fba6-40f1-a901-07b924594272\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.scheduler - INFO - Receive client connection: Client-fcf3b862-43d9-11ea-843d-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.10:41583', name: tcp://10.3.0.10:41583, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.10:41583\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.11:35845', name: tcp://10.3.0.11:35845, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.11:35845\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.12:33393', name: tcp://10.3.0.12:33393, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.12:33393\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.13:32861', name: tcp://10.3.0.13:32861, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.13:32861\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.14:34091', name: tcp://10.3.0.14:34091, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.14:34091\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.15:41959', name: tcp://10.3.0.15:41959, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.15:41959\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.16:40787', name: tcp://10.3.0.16:40787, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.16:40787\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.17:35795', name: tcp://10.3.0.17:35795, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.17:35795\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.18:35841', name: tcp://10.3.0.18:35841, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.18:35841\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.19:37979', name: tcp://10.3.0.19:37979, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.19:37979\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.20:35541', name: tcp://10.3.0.20:35541, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.20:35541\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.21:45521', name: tcp://10.3.0.21:45521, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.21:45521\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.22:37321', name: tcp://10.3.0.22:37321, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.22:37321\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.23:41599', name: tcp://10.3.0.23:41599, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.23:41599\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.24:46569', name: tcp://10.3.0.24:46569, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.24:46569\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.25:41407', name: tcp://10.3.0.25:41407, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.25:41407\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.26:42475', name: tcp://10.3.0.26:42475, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.26:42475\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.27:40517', name: tcp://10.3.0.27:40517, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.27:40517\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.29:40539', name: tcp://10.3.0.29:40539, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.29:40539\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.30:36267', name: tcp://10.3.0.30:36267, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.30:36267\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.5:37827', name: tcp://10.3.0.5:37827, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.5:37827\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.6:37499', name: tcp://10.3.0.6:37499, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.6:37499\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.7:46429', name: tcp://10.3.0.7:46429, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.7:46429\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.8:40649', name: tcp://10.3.0.8:40649, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.8:40649\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.9:33145', name: tcp://10.3.0.9:33145, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.9:33145\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.nanny - WARNING - Worker process still alive after 1 seconds, killing\\ndistributed.diskutils - INFO - Found stale lock file and directory '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-zjma6mzw', purging\\ndistributed.diskutils - ERROR - Failed to clean up lingering worker directories in path: %s \\nTraceback (most recent call last):\\n  File \\\"/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/distributed/diskutils.py\\\", line 237, in new_work_dir\\n    self._purge_leftovers()\\n  File \\\"/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/distributed/diskutils.py\\\", line 168, in _purge_leftovers\\n    if self._check_lock_or_purge(path):\\n  File \\\"/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/distributed/diskutils.py\\\", line 203, in _check_lock_or_purge\\n    lock.acquire()\\n  File \\\"/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/distributed/locket.py\\\", line 196, in acquire\\n    self._lock.acquire(self._timeout, self._retry_period)\\n  File \\\"/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/distributed/locket.py\\\", line 125, in acquire\\n    lock.acquire(timeout, retry_period)\\n  File \\\"/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/distributed/locket.py\\\", line 167, in acquire\\n    self._file = open(self._path, \\\"wb\\\")\\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-7ir2r498.dirlock'\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.12:36823', name: tcp://10.3.0.12:36823, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.12:36823\\ndistributed.core - INFO - Starting established connection\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:45813\\ndistributed.worker - INFO -          Listening to:       tcp://10.3.0.5:45813\\ndistributed.worker - INFO -          dashboard at:             10.3.0.5:37183\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                         16\\ndistributed.worker - INFO -                Memory:                  118.27 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-mpfs9dcs\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.9:39287', name: tcp://10.3.0.9:39287, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.9:39287\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.25:43415', name: tcp://10.3.0.25:43415, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:43415\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.20:46505', name: tcp://10.3.0.20:46505, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.20:46505\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.19:32869', name: tcp://10.3.0.19:32869, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.19:32869\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.10:34171', name: tcp://10.3.0.10:34171, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:34171\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.11:43279', name: tcp://10.3.0.11:43279, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.11:43279\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.5:45813', name: tcp://10.3.0.5:45813, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:45813\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.27:44225', name: tcp://10.3.0.27:44225, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.27:44225\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.6:35861', name: tcp://10.3.0.6:35861, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:35861\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.22:34797', name: tcp://10.3.0.22:34797, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:34797\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.26:44729', name: tcp://10.3.0.26:44729, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.26:44729\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.7:46461', name: tcp://10.3.0.7:46461, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.7:46461\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.8:46841', name: tcp://10.3.0.8:46841, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:46841\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.23:37791', name: tcp://10.3.0.23:37791, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.23:37791\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.13:37137', name: tcp://10.3.0.13:37137, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.13:37137\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.29:46413', name: tcp://10.3.0.29:46413, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.29:46413\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.17:37039', name: tcp://10.3.0.17:37039, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:37039\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.30:37455', name: tcp://10.3.0.30:37455, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.30:37455\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.21:37529', name: tcp://10.3.0.21:37529, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.21:37529\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.24:41779', name: tcp://10.3.0.24:41779, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:41779\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.16:42863', name: tcp://10.3.0.16:42863, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.16:42863\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.15:40851', name: tcp://10.3.0.15:40851, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:40851\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.14:34703', name: tcp://10.3.0.14:34703, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:34703\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.18:43585', name: tcp://10.3.0.18:43585, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.18:43585\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Remove client Client-fcf3b862-43d9-11ea-843d-000d3ad498da\\ndistributed.scheduler - INFO - Remove client Client-fcf3b862-43d9-11ea-843d-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-fcf3b862-43d9-11ea-843d-000d3ad498da\\n[I 03:30:18.871 LabApp] Kernel restarted: 13227a22-fba6-40f1-a901-07b924594272\\ndistributed.scheduler - INFO - Receive client connection: Client-030d0d12-43da-11ea-8482-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\\nRequirement already up-to-date: matplotlib in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (3.1.2)\\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.18.1)\\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (0.10.0)\\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.4.6)\\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.1.0)\\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.8.1)\\nRequirement already satisfied, skipping upgrade: six in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\\nRequirement already satisfied, skipping upgrade: setuptools in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200119)\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.10:34171', name: tcp://10.3.0.10:34171, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.10:34171\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.11:43279', name: tcp://10.3.0.11:43279, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.11:43279\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.12:36823', name: tcp://10.3.0.12:36823, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.12:36823\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.13:37137', name: tcp://10.3.0.13:37137, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.13:37137\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.14:34703', name: tcp://10.3.0.14:34703, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.14:34703\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.15:40851', name: tcp://10.3.0.15:40851, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.15:40851\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.16:42863', name: tcp://10.3.0.16:42863, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.16:42863\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.17:37039', name: tcp://10.3.0.17:37039, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.17:37039\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.18:43585', name: tcp://10.3.0.18:43585, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.18:43585\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.19:32869', name: tcp://10.3.0.19:32869, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.19:32869\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.20:46505', name: tcp://10.3.0.20:46505, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.20:46505\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.21:37529', name: tcp://10.3.0.21:37529, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.21:37529\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.22:34797', name: tcp://10.3.0.22:34797, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.22:34797\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.23:37791', name: tcp://10.3.0.23:37791, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.23:37791\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.24:41779', name: tcp://10.3.0.24:41779, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.24:41779\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.25:43415', name: tcp://10.3.0.25:43415, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.25:43415\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.26:44729', name: tcp://10.3.0.26:44729, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.26:44729\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.27:44225', name: tcp://10.3.0.27:44225, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.27:44225\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.29:46413', name: tcp://10.3.0.29:46413, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.29:46413\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.30:37455', name: tcp://10.3.0.30:37455, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.30:37455\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.5:45813', name: tcp://10.3.0.5:45813, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.5:45813\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.6:35861', name: tcp://10.3.0.6:35861, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.6:35861\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.7:46461', name: tcp://10.3.0.7:46461, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.7:46461\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.8:46841', name: tcp://10.3.0.8:46841, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.8:46841\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.9:39287', name: tcp://10.3.0.9:39287, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.9:39287\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.worker - INFO - Stopping worker at tcp://10.3.0.5:45813\\ndistributed.nanny - INFO - Worker closed\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.12:33417', name: tcp://10.3.0.12:33417, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.12:33417\\ndistributed.core - INFO - Starting established connection\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.10:45261', name: tcp://10.3.0.10:45261, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:45261\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.7:33605', name: tcp://10.3.0.7:33605, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.7:33605\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:35693\\ndistributed.worker - INFO -          Listening to:       tcp://10.3.0.5:35693\\ndistributed.worker - INFO -          dashboard at:             10.3.0.5:36969\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.16:35315', name: tcp://10.3.0.16:35315, memory: 0, processing: 0>\\ndistributed.worker - INFO -               Threads:                         16\\ndistributed.worker - INFO -                Memory:                  118.27 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-iodfbgag\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.16:35315\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.23:39215', name: tcp://10.3.0.23:39215, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.23:39215\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.13:44897', name: tcp://10.3.0.13:44897, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.13:44897\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.6:40057', name: tcp://10.3.0.6:40057, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:40057\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.24:46453', name: tcp://10.3.0.24:46453, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:46453\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.11:45159', name: tcp://10.3.0.11:45159, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.11:45159\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.25:44941', name: tcp://10.3.0.25:44941, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:44941\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.19:35239', name: tcp://10.3.0.19:35239, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.19:35239\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.14:33685', name: tcp://10.3.0.14:33685, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:33685\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.9:39475', name: tcp://10.3.0.9:39475, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.9:39475\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.27:45453', name: tcp://10.3.0.27:45453, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.27:45453\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.15:38545', name: tcp://10.3.0.15:38545, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:38545\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.5:35693', name: tcp://10.3.0.5:35693, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:35693\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.18:37351', name: tcp://10.3.0.18:37351, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.18:37351\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.30:45627', name: tcp://10.3.0.30:45627, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.30:45627\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.8:43907', name: tcp://10.3.0.8:43907, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:43907\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.22:39225', name: tcp://10.3.0.22:39225, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:39225\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.29:33735', name: tcp://10.3.0.29:33735, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.29:33735\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.20:44343', name: tcp://10.3.0.20:44343, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.20:44343\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.17:35233', name: tcp://10.3.0.17:35233, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:35233\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.21:40173', name: tcp://10.3.0.21:40173, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.21:40173\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.26:43539', name: tcp://10.3.0.26:43539, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.26:43539\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Receive client connection: Client-0647f83a-43da-11ea-8482-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove client Client-0647f83a-43da-11ea-8482-000d3ad498da\\ndistributed.scheduler - INFO - Remove client Client-030d0d12-43da-11ea-8482-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-0647f83a-43da-11ea-8482-000d3ad498da\\ndistributed.scheduler - INFO - Close client connection: Client-030d0d12-43da-11ea-8482-000d3ad498da\\n[I 03:30:34.412 LabApp] Kernel restarted: 13227a22-fba6-40f1-a901-07b924594272\\n[I 03:30:41.121 LabApp] Kernel interrupted: 13227a22-fba6-40f1-a901-07b924594272\\n[I 03:30:41.240 LabApp] Kernel interrupted: 13227a22-fba6-40f1-a901-07b924594272\\ndistributed.scheduler - INFO - Receive client connection: Client-1164dfe4-43da-11ea-84ce-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\\nRequirement already up-to-date: matplotlib in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (3.1.2)\\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.1.0)\\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.8.1)\\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (2.4.6)\\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (0.10.0)\\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from matplotlib) (1.18.1)\\nRequirement already satisfied, skipping upgrade: setuptools in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200119)\\nRequirement already satisfied, skipping upgrade: six>=1.5 in /azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.10:45261', name: tcp://10.3.0.10:45261, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.10:45261\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.11:45159', name: tcp://10.3.0.11:45159, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.11:45159\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.12:33417', name: tcp://10.3.0.12:33417, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.12:33417\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.13:44897', name: tcp://10.3.0.13:44897, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.13:44897\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.14:33685', name: tcp://10.3.0.14:33685, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.14:33685\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.15:38545', name: tcp://10.3.0.15:38545, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.15:38545\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.16:35315', name: tcp://10.3.0.16:35315, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.16:35315\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.17:35233', name: tcp://10.3.0.17:35233, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.17:35233\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.18:37351', name: tcp://10.3.0.18:37351, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.18:37351\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.19:35239', name: tcp://10.3.0.19:35239, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.19:35239\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.20:44343', name: tcp://10.3.0.20:44343, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.20:44343\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.21:40173', name: tcp://10.3.0.21:40173, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.21:40173\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.22:39225', name: tcp://10.3.0.22:39225, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.22:39225\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.23:39215', name: tcp://10.3.0.23:39215, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.23:39215\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.24:46453', name: tcp://10.3.0.24:46453, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.24:46453\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.25:44941', name: tcp://10.3.0.25:44941, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.25:44941\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.26:43539', name: tcp://10.3.0.26:43539, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.26:43539\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.27:45453', name: tcp://10.3.0.27:45453, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.27:45453\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.29:33735', name: tcp://10.3.0.29:33735, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.29:33735\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.30:45627', name: tcp://10.3.0.30:45627, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.30:45627\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.5:35693', name: tcp://10.3.0.5:35693, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.5:35693\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.6:40057', name: tcp://10.3.0.6:40057, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.6:40057\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.7:33605', name: tcp://10.3.0.7:33605, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.7:33605\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.8:43907', name: tcp://10.3.0.8:43907, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.8:43907\\ndistributed.scheduler - INFO - Remove worker <Worker 'tcp://10.3.0.9:39475', name: tcp://10.3.0.9:39475, memory: 0, processing: 0>\\ndistributed.core - INFO - Removing comms to tcp://10.3.0.9:39475\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.worker - INFO - Stopping worker at tcp://10.3.0.5:35693\\ndistributed.nanny - INFO - Worker closed\\ndistributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\\ndistributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:33265\\ndistributed.worker - INFO -          Listening to:       tcp://10.3.0.5:33265\\ndistributed.worker - INFO -          dashboard at:             10.3.0.5:39999\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                         16\\ndistributed.worker - INFO -                Memory:                  118.27 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/azureml-dask/azureml/dask-dask-demo_1580440649_506153b5/mounts/workspaceblobstore/azureml/dask-dask-demo_1580440649_506153b5/worker-qe_fonxu\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.12:39963', name: tcp://10.3.0.12:39963, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.12:39963\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.24:37235', name: tcp://10.3.0.24:37235, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:37235\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.10:33003', name: tcp://10.3.0.10:33003, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:33003\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.25:33533', name: tcp://10.3.0.25:33533, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:33533\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.23:35975', name: tcp://10.3.0.23:35975, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.23:35975\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.13:35001', name: tcp://10.3.0.13:35001, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.13:35001\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.20:45555', name: tcp://10.3.0.20:45555, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.20:45555\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.6:46703', name: tcp://10.3.0.6:46703, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:46703\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.27:36835', name: tcp://10.3.0.27:36835, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.27:36835\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.7:33331', name: tcp://10.3.0.7:33331, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.7:33331\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.30:34291', name: tcp://10.3.0.30:34291, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.30:34291\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.26:38477', name: tcp://10.3.0.26:38477, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.26:38477\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.5:33265', name: tcp://10.3.0.5:33265, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:33265\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.3.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.22:42613', name: tcp://10.3.0.22:42613, memory: 0, processing: 0>\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:42613\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.14:37667', name: tcp://10.3.0.14:37667, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:37667\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.9:42099', name: tcp://10.3.0.9:42099, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.9:42099\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.21:38713', name: tcp://10.3.0.21:38713, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.21:38713\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.19:39779', name: tcp://10.3.0.19:39779, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.19:39779\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.15:39511', name: tcp://10.3.0.15:39511, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:39511\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.29:34883', name: tcp://10.3.0.29:34883, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.29:34883\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.16:34799', name: tcp://10.3.0.16:34799, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.16:34799\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.8:40461', name: tcp://10.3.0.8:40461, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:40461\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.11:33295', name: tcp://10.3.0.11:33295, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.11:33295\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.18:41771', name: tcp://10.3.0.18:41771, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.18:41771\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.3.0.17:45787', name: tcp://10.3.0.17:45787, memory: 0, processing: 0>\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:45787\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Receive client connection: Client-1b754492-43da-11ea-84ce-000d3ad498da\\ndistributed.core - INFO - Starting established connection\\n[I 03:31:07.194 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\n[I 03:33:07.999 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:35:08.805 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:35:36.912 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:37:37.816 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:39:38.739 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:41:39.718 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\n[I 03:43:44.715 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 03:45:47.972 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n[I 04:07:53.769 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 19.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\n[I 04:23:55.914 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\ndistributed.worker - INFO - Run out-of-band function 'start_tracker'\\n/azureml-envs/azureml_d2ba0c6c1f746ec598174ceaba2cb94d/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\\n  if getattr(data, 'base', None) is not None and \\\\\\ndistributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\\n[04:24:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\\n[04:24:43] WARNING: /workspace/src/learner.cc:622: Tree method is automatically selected to be 'approx' for distributed training.\\n[04:24:43] Tree method is automatically selected to be 'approx' for distributed training.\\n[I 04:25:57.587 LabApp] Saving file at /codefileshare/Users/cody/dasky/demo-cpu-100GB-csvs.ipynb\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter/Dashboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for scheduler node's ip\n",
      "\n",
      "Setting up ports...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# port to forward the dask dashboard to on the compute instance\n",
    "# we do not use 8787 because it is already in use \n",
    "dashboard_port = 4242\n",
    "jupyter_port   = 9999\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while run.get_status() != 'Canceled' and 'scheduler' not in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if run.get_status() == 'Canceled':\n",
    "    print('\\nRun was canceled')\n",
    "else:\n",
    "    print(f'\\nSetting up ports...')\n",
    "    os.system(f'killall socat') # kill all socat processes - cleans up previous port forward setups \n",
    "    os.system(f'setsid socat tcp-listen:{dashboard_port},reuseaddr,fork tcp:{run.get_metrics()[\"dashboard\"]} &')\n",
    "    os.system(f'setsid socat tcp-listen:{jupyter_port},reuseaddr,fork tcp:{run.get_metrics()[\"jupyter\"]} &')\n",
    "    print(f'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://chromebook-4242.uksouth.instances.azureml.net/status\">Dashboard link</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the dashboard link \n",
    "dashboard_url = f'https://{socket.gethostname()}-{dashboard_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/status'\n",
    "HTML(f'<a href=\"{dashboard_url}\">Dashboard link</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://chromebook-9999.uksouth.instances.azureml.net/lab?token=988129fa43d811eaa13f000d3ad498da\">Jupyter link</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the jupyter link \n",
    "jupyter_url = f'https://{socket.gethostname()}-{jupyter_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/lab?token={run.get_metrics()[\"token\"]}'\n",
    "HTML(f'<a href=\"{jupyter_url}\">Jupyter link</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
