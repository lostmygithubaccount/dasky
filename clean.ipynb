{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Python with Azure ML and Dask\n",
    "\n",
    "![Describe gif](media/describe5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "This notebook assumes you are using an Azure ML Compute Instance with the default kernel `azureml_py36`. This contains many unneccesary packages. If you want to avoid a long image build time, you may want to create a new conda environment with the minimal packages needed for your scenario. \n",
    "\n",
    "It is important that the local environment matches the remote environment to avoid mismatch issues when submitting commands to the remote cluster. To help with this, we will use Azure ML Environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade azureml-sdk[notebooks] azureml-dataprep[fuse] fastparquet pyarrow dask distributed lz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall azureml-samples -y # stupid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML('<script>Jupyter.notebook.kernel.restart()</script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('sudo cp /etc/nginx/nginx.conf setup/temp.conf') # stupid\n",
    "\n",
    "nginx = ''\n",
    "\n",
    "with open('setup/temp.conf') as f:\n",
    "    for line in f.readlines():\n",
    "        if 'websocket/|/ws/' in line:\n",
    "            nginx += line.replace('websocket/|/ws/', 'websocket/|/ws')\n",
    "        else:\n",
    "            nginx += line\n",
    "       \n",
    "with open('setup/temp2.conf', 'w') as f:\n",
    "    f.write(nginx)\n",
    "    \n",
    "os.system('sudo mv setup/temp2.conf /etc/nginx/nginx.conf')\n",
    "os.system('sudo service nginx restart')\n",
    "os.system('rm setup/temp.conf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all packages used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dask\n",
    "import glob\n",
    "import socket\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Setup\n",
    "\n",
    "Get the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment \n",
    "\n",
    "Create the environment to be used on the remote cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'dask-env'\n",
    "\n",
    "if env_name not in ws.environments:\n",
    "    env = Environment.from_existing_conda_environment(env_name, 'azureml_py36')\n",
    "    env.python.conda_dependencies.add_pip_package('mpi4py') # needed for remote cluster\n",
    "    env = env.register(ws)\n",
    "else:\n",
    "    env = ws.environments[env_name]\n",
    "    \n",
    "env.name, env.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "\n",
    "Create the dataset to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'weather-files'\n",
    "data_paths   = ['https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/**']\n",
    "\n",
    "if dataset_name not in ws.datasets:\n",
    "    ds = Dataset.File.from_files(data_paths, validate=False)\n",
    "    ds = ds.register(ws, dataset_name)\n",
    "else:\n",
    "    ds = ws.datasets[dataset_name]\n",
    "    \n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VM pool\n",
    "\n",
    "Create Azure ML VM pool for creating remote dask cluster(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_name = 'dask-pool'\n",
    "\n",
    "if pool_name not in ws.compute_targets:\n",
    "    # create config for Azure ML cluster\n",
    "    # change properties as needed\n",
    "    config = AmlCompute.provisioning_configuration(\n",
    "             vm_size                 = 'STANDARD_DS13_V2',\n",
    "             max_nodes               = 100,\n",
    "             vnet_resourcegroup_name = ws.resource_group,\n",
    "             vnet_name               = 'dask-vnet',\n",
    "             subnet_name             = 'default'\n",
    "    )\n",
    "    \n",
    "    ct = ComputeTarget.create(ws, pool_name, config)\n",
    "    ct.wait_for_completion(show_output=True)    \n",
    "else:\n",
    "    ct = ws.compute_targets[pool_name]\n",
    "    \n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "\n",
    "This uses an [Azure Open Dataset](https://azure.microsoft.com/services/open-datasets/catalog/) of [NOAA Integrated Surface Data (ISD)](https://azure.microsoft.com/services/open-datasets/catalog/noaa-integrated-surface-data/) containing worldwide hourly weather data such as temperature, precipitation, and wind. \n",
    "\n",
    "Expanded in memory, the full dataset is ~660 GB. It is stored in compressed parquet files in a blob container partitioned by year and month. The dataset is updated daily. Compressed, the files for the dataset are ~8 GB. Uncompressed, the files for the dataset are ~150-200 GB.  \n",
    "\n",
    "Specific years and months can be specified by `year=*/month=*/part-*.snappy.parquet`. \n",
    "\n",
    "The data begins in 2008 and contains 1 file per month. Each file can contain ~5 GB of data when in a dataframe in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/noaa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('sudo chmod 777 /mnt'); # stupid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ds.download(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_path}/year=*/month=*/*.parquet', recursive=True)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_path}/year=2019/month=12/*.parquet', recursive=True) # 1 month of data\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(files[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.datetime.dt.floor('d')).mean()\n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(2019, 12, 1), datetime(2019, 12, 31)])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling with Dask\n",
    "\n",
    "The first step in scaling up with Dask is to simply get a bigger VM. For non-GPU Compute Instances, the max is the `STANDARD_DS15_V2` with 20 cores and 140 GB of RAM. This is suitable for interactive querying and data preparation on about 1 year of the weather data, but not on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_port = 4242\n",
    "\n",
    "c = Client(dashboard_address=f':{dashboard_port}')\n",
    "c.restart()\n",
    "\n",
    "print(f'\\n\\n{c}')\n",
    "\n",
    "# need to get the dashboard link \n",
    "dashboard_url = f'https://{socket.gethostname()}-{dashboard_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/status'\n",
    "HTML(f'<a href=\"{dashboard_url}\">Dashboard link</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take some data\n",
    "\n",
    "Take some data. The below cells may fail on smaller VMs. 1 year works semi-reliably on a `STANDARD_DS15_V2`, although computing the means fails sometimes.\n",
    "\n",
    "You can use the dashboard to understand what is going on with this VM being used as a \"local\" cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2017\n",
    "end   = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.delayed(dd.read_parquet)([f'{data_path}/year={year}/month=*/*.parquet' for year in range(start, end+1)], engine='pyarrow').compute().persist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum().compute()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.datetime.dt.floor('d')).mean().compute() # slow, prone to error \n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(start, 1, 1), datetime(end, 12, 31)])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale with Dask and Azure ML\n",
    "\n",
    "Scale up to using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name   = 'dask'\n",
    "mount      = False\n",
    "data_path  = '/mnt/noaa'\n",
    "dset_input = ds.as_named_input('data').as_mount(data_path) if mount else ds.as_named_input('data').as_download(data_path)\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ws.get_default_datastore(),\n",
    "    #'--script'   : 'run.py' # run code in run.py on cluster and teardown (batch processing)\n",
    "}\n",
    "\n",
    "est = Estimator('setup', \n",
    "                compute_target          = ct, \n",
    "                entry_script            = 'start.py', \n",
    "                environment_definition  = env, \n",
    "                script_params           = script_params,\n",
    "                inputs                  = [dset_input],\n",
    "                node_count              = 35,\n",
    "                distributed_training    = MpiConfiguration()\n",
    "               )\n",
    "\n",
    "#run = next(ws.experiments[exp_name].get_runs()) # use this to get existing run (if kernel restarted, etc)\n",
    "run = Experiment(ws, exp_name).submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for cluster to setup\n",
    "\n",
    "Wait for the cluster to spin up, the run to start, and the `start.py` script to complete. At this point, information needed for connecting the cluster will be logged back to the run.\n",
    "\n",
    "The below cell waits for that information to be logged to the run, sets up port forwarding for the Dask dashboard, and prints the url. Make sure to check out the dashboard as you run through the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# port to forward the dask dashboard to on the compute instance\n",
    "# we do not use 8787 because it is already in use \n",
    "dashboard_port = 4343\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while run.get_status() != 'Canceled' and 'scheduler' not in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if run.get_status() == 'Canceled':\n",
    "    print('Run was canceled')\n",
    "else:\n",
    "    print(f'Setting up port forwarding...')\n",
    "    os.system(f'killall socat') # kill all socat processes - cleans up previous port forward setups \n",
    "    os.system(f'setsid socat tcp-listen:{dashboard_port},reuseaddr,fork tcp:{run.get_metrics()[\"dashboard\"]} &')\n",
    "    print(f'Cluster is ready to use.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(f'tcp://{run.get_metrics()[\"scheduler\"]}')\n",
    "c.restart()\n",
    "\n",
    "print(f'\\n\\n{c}')\n",
    "\n",
    "# need to get the dashboard link \n",
    "dashboard_url = f'https://{socket.gethostname()}-{dashboard_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/status'\n",
    "HTML(f'<a href=\"{dashboard_url}\">Dashboard link</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dask.delayed(glob.glob)(f'{data_path}/year=*/month=*/*.parquet', recursive=True).compute()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.from_delayed([dask.delayed(pd.read_parquet)(file) for file in files])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum().compute()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.datetime.dt.floor('d')).mean().compute()\n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(2008, 1, 1), datetime(2019, 12, 31)])\n",
    "    plt.grid()\n",
    "    \n",
    "    # optionally, log the image to the run\n",
    "    run.log_image(f'mean_{col}', plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.set_index(dd.to_datetime(df.datetime).dt.floor('d'), sorted=False).persist() # persist and sort data by day "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert any Pandas-like Dask data prep code \n",
    "df['temperature'] = df['temperature']*(9/5)+32       # 'Merica\n",
    "means['temperature'] = means['temperature']*(9/5)+32 # skip recomputing this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "means.temperature.plot(color='b')\n",
    "plt.title('Real average of temperature')\n",
    "plt.xlim([datetime(2008, 1, 1), datetime(2019, 12, 31)])\n",
    "plt.ylabel('Temperature in \\u00B0F')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data\n",
    "\n",
    "**Important:** this is slow and will put 200 GB of CSVs in your default storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dask.delayed(df.to_csv)(f'/{run.get_metrics()[\"datastore\"]}/dask/output/noaa/data-part-*.csv').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "dset = Dataset.File.from_files((ws.get_default_datastore(), '/dask/output/noaa/**/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dset.register(ws, 'real-weather-files')\n",
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End the run\n",
    "\n",
    "Cluster will return to 0 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
