{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Python with Azure ML and Dask\n",
    "\n",
    "![Describe gif](media/describe.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "This notebook assumes you are using an Azure ML Compute Instance with the default kernel `azureml_py36`. This contains many unneccesary packages. If you want to avoid a long image build time, you may want to create a new conda environment with the minimal packages needed for your scenario. \n",
    "\n",
    "It is important that the local environment matches the remote environment to avoid mismatch issues when submitting commands to the remote cluster. To help with this, we will use Azure ML Environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade dask[complete] adlfs lz4 distributed fastparquet pyarrow azureml-sdk[notebooks] azureml-dataprep[fuse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall azureml-samples -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML('<script>Jupyter.notebook.kernel.restart()</script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('sudo cp /etc/nginx/nginx.conf setup/temp.conf') # stupid\n",
    "\n",
    "nginx = ''\n",
    "\n",
    "with open('setup/temp.conf') as f:\n",
    "    for line in f.readlines():\n",
    "        if 'websocket/|/ws/' in line:\n",
    "            nginx += line.replace('websocket/|/ws/', 'websocket/|/ws')\n",
    "        else:\n",
    "            nginx += line\n",
    "       \n",
    "with open('setup/temp2.conf', 'w') as f:\n",
    "    f.write(nginx)\n",
    "    \n",
    "os.system('sudo mv setup/temp2.conf /etc/nginx/nginx.conf')\n",
    "os.system('sudo service nginx restart')\n",
    "os.system('rm setup/temp.conf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all packages used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dask\n",
    "import glob\n",
    "import time\n",
    "import socket\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Setup\n",
    "\n",
    "Get the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for a ADLS gen2 account I have provisioned with data. It should be read-able publicly.\n",
    "\n",
    "You **should not** keep storage account keys in plain text format, and you definitely should not upload them to github in a public repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_OPTIONS = {\n",
    "    'account_name': 'data4dask', \n",
    "    'account_key' : 'V1a2dz8bOH+zXsOXk9YbkJGFxabjlLffXLeQ0+sGh4+vsT79fgPyAU/TNkQOg10Vt7NxqRT8I/9iftiwcfqXHA=='\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment \n",
    "\n",
    "Create the environment to be used on the remote cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'dask-env'\n",
    "\n",
    "if env_name not in ws.environments:\n",
    "    env = Environment.from_existing_conda_environment(env_name, 'azureml_py36')\n",
    "    env.python.conda_dependencies.add_pip_package('mpi4py') # needed for remote cluster\n",
    "    env = env.register(ws)\n",
    "else:\n",
    "    env = ws.environments[env_name]\n",
    "    \n",
    "env.name, env.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "\n",
    "Create the dataset to be used. This will only be used locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'noaa-isd-files'\n",
    "data_paths   = ['https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=*/month=*/*.parquet']\n",
    "local_path   = '/mnt/data/noaa/isd'\n",
    "remote_path  = '/datasets/noaa/isd'\n",
    "\n",
    "if dataset_name not in ws.datasets:\n",
    "    ds = Dataset.File.from_files(data_paths, validate=False)\n",
    "    # begin stupid \n",
    "    os.system('sudo chmod 777 /mnt') # stupid \n",
    "    ds.download(local_path)\n",
    "    ws.get_default_datastore().upload(local_path, remote_path)\n",
    "    ds = Dataset.File.from_files((ws.get_default_datastore(), remote_path))\n",
    "    # end stupid\n",
    "    ds = ds.register(ws, dataset_name)\n",
    "else:\n",
    "    ds = ws.datasets[dataset_name]\n",
    "    \n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VM pool\n",
    "\n",
    "Create Azure ML VM pool for creating remote dask cluster(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_name = 'raspberrypis'\n",
    "\n",
    "if pool_name not in ws.compute_targets:\n",
    "    # create config for Azure ML cluster\n",
    "    # change properties as needed\n",
    "    config = AmlCompute.provisioning_configuration(\n",
    "             vm_size                 = 'STANDARD_DS13_V2',  # 8 vCPUS 56 GB RAM 112 GB disk \n",
    "             max_nodes               = 100,\n",
    "             vnet_resourcegroup_name = ws.resource_group,   # replace if needed\n",
    "             vnet_name               = 'dialup-network',    # replace if needed\n",
    "             subnet_name             = 'default'            # replace if needed\n",
    "    )\n",
    "    \n",
    "    ct = ComputeTarget.create(ws, pool_name, config)\n",
    "    ct.wait_for_completion(show_output=True)    \n",
    "else:\n",
    "    ct = ws.compute_targets[pool_name]\n",
    "    \n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup cluster\n",
    "\n",
    "Start the run now. The first time, this will take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name   = 'dask-interactive'\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ws.get_default_datastore(),\n",
    "    #'--script'   : 'run.py' # run code in run.py on cluster and teardown (batch processing)\n",
    "}\n",
    "\n",
    "est = Estimator('setup', \n",
    "                compute_target          = ct, \n",
    "                entry_script            = 'start.py',          # sets up Dask cluster\n",
    "                environment_definition  = env,                 # use same env as local\n",
    "                script_params           = script_params,       \n",
    "                node_count              = 20,                  # 20 nodes -> 160 vCPUs, 1 TB RAM\n",
    "                distributed_training    = MpiConfiguration()\n",
    "               )\n",
    "\n",
    "#run = next(ws.experiments[exp_name].get_runs()) # use this to get existing run (if kernel restarted, etc)\n",
    "run = Experiment(ws, exp_name).submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "\n",
    "This uses an [Azure Open Dataset](https://azure.microsoft.com/services/open-datasets/catalog/) of [NOAA Integrated Surface Data (ISD)](https://azure.microsoft.com/services/open-datasets/catalog/noaa-integrated-surface-data/) containing worldwide hourly weather data such as temperature, precipitation, and wind. \n",
    "\n",
    "Expanded in memory, the full dataset is ~660 GB. It is stored in compressed parquet files in a blob container partitioned by year and month. The dataset is updated daily. Compressed, the files for the dataset are ~8 GB. Uncompressed, the files for the dataset are ~150-200 GB.  \n",
    "\n",
    "Specific years and months can be specified by `year=*/month=*/part-*.snappy.parquet`. \n",
    "\n",
    "The data begins in 2008 and contains 1 file per month. Each file can contain ~5 GB of data when in a dataframe in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time files = glob.glob(f'{local_path}/year=*/month=*/*.parquet', recursive=True)\n",
    "files[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{local_path}/year=2019/month=12/*.parquet', recursive=True) # 1 month of data\n",
    "files[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(files[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.datetime.dt.floor('d')).mean()\n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(2019, 12, 1), datetime(2019, 12, 31)])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling with Dask 'locally'\n",
    "\n",
    "The first step in scaling up with Dask is to simply get a bigger VM. For non-GPU Compute Instances, the max is the `STANDARD_DS15_V2` with 20 cores and 140 GB of RAM. This is suitable for interactive querying and data preparation on about 1 year of the weather data, but not on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_port = 3210\n",
    "\n",
    "c = Client(dashboard_address=f':{dashboard_port}')\n",
    "print(f'\\n\\n{c}')\n",
    "\n",
    "# need to get the dashboard link \n",
    "dashboard_url = f'https://{socket.gethostname()}-{dashboard_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/status'\n",
    "HTML(f'<a href=\"{dashboard_url}\">Dashboard link</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take some data\n",
    "\n",
    "Take some data. The below cells may fail on smaller VMs. 1 year works semi-reliably on a `STANDARD_DS15_V2`, although computing the means fails sometimes.\n",
    "\n",
    "You can use the dashboard to understand what is going on with this VM being used as a \"local\" cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(f'{local_path}/year=201[7-8]/month=*/*.parquet', engine='pyarrow') # takes 2 years of data - adjust if needed\n",
    "%time df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum().compute()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.datetime.dt.floor('d')).mean().compute() # slow, prone to error \n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    #plt.xlim([datetime(2017, 1, 1), datetime(2018, 12, 31)])  # adjust if needed\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up with Dask and Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# port to forward the dask dashboard to on the compute instance\n",
    "# we do not use 8787 because it is already in use \n",
    "dashboard_port = 4242\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while run.get_status() != 'Canceled' and 'scheduler' not in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if run.get_status() == 'Canceled':\n",
    "    print('Run was canceled')\n",
    "else:\n",
    "    print(f'Setting up port forwarding...')\n",
    "    os.system(f'killall socat') # kill all socat processes - cleans up previous port forward setups \n",
    "    os.system(f'setsid socat tcp-listen:{dashboard_port},reuseaddr,fork tcp:{run.get_metrics()[\"dashboard\"]} &')\n",
    "    print(f'Cluster is ready to use.')\n",
    "\n",
    "c = Client(f'tcp://{run.get_metrics()[\"scheduler\"]}')\n",
    "\n",
    "print(f'\\n\\n{c}')\n",
    "\n",
    "c.restart()\n",
    "\n",
    "# need to get the dashboard link \n",
    "dashboard_url = f'https://{socket.gethostname()}-{dashboard_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/status'\n",
    "HTML(f'<a href=\"{dashboard_url}\">Dashboard link</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{local_path}/year=*/month=*/*.parquet', recursive=True)\n",
    "files = [file.replace('/mnt/data', 'abfs://datasets') for file in files if '2019' not in file]\n",
    "files[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.delayed(dd.read_parquet)(files, engine='pyarrow', storage_options=STORAGE_OPTIONS).compute()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(dd.to_datetime(df.datetime).dt.floor('d'), sorted=False).persist() # persist and sort data by day \n",
    "#df = df.persist()\n",
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum().compute()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.index).mean().compute()\n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(2008, 1, 1), datetime(2018, 12, 31)])\n",
    "    plt.grid()\n",
    "    \n",
    "    # optionally, log the image to the run\n",
    "    run.log_image(f'mean_{col}', plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert any Pandas-like Dask data prep code \n",
    "df['temperature'] = df['temperature']*(9/5)+32       # 'Merica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data\n",
    "\n",
    "**Important:** write back to ADLS gen 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = dask.delayed(df.to_parquet)(f'abfs://outputs/noaa/isd_out.parquet', compression='lz4', storage_options=STORAGE_OPTIONS).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End the run\n",
    "\n",
    "Cluster will return to 0 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a batch job (WIP)\n",
    "\n",
    "Now that we know what data processing we want to do, let's do it in a batch instead of interactively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup/run.py\n",
    "\n",
    "import time\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from azureml.core import Run\n",
    "from dask.distributed import Client\n",
    "\n",
    "c = Client(f'tcp://{run.get_metrics()[\"scheduler\"]}')\n",
    "\n",
    "Run.get_context().log('client', c)\n",
    "c.restart()\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "Run.get_context().log('dask_time (s)', round(t2-t1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name   = 'dask-batch'\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ws.get_default_datastore(),\n",
    "    '--script'   : 'run.py' # run code in run.py on cluster and teardown (batch processing)\n",
    "}\n",
    "\n",
    "est = Estimator('setup', \n",
    "                compute_target          = ct, \n",
    "                entry_script            = 'start.py',          # sets up Dask cluster\n",
    "                environment_definition  = env,                 # use same env as local\n",
    "                script_params           = script_params,       \n",
    "                node_count              = 20,                  # 20 nodes -> 160 vCPUs, 1 TB RAM\n",
    "                distributed_training    = MpiConfiguration()\n",
    "               )\n",
    "\n",
    "#run = next(ws.experiments[exp_name].get_runs()) # use this to get existing run (if kernel restarted, etc)\n",
    "run = Experiment(ws, exp_name).submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
